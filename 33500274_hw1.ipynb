{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsuj1th/Colab/blob/main/33500274_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAxczUGVN-Jc"
      },
      "source": [
        "#### CSCE 670 :: Information Storage & Retrieval :: Texas A&M University :: Spring 2025\n",
        "\n",
        "\n",
        "# Homework 1:  OASIS Search Engine, The Beginning\n",
        "\n",
        "### 100 points [4% of your final grade]\n",
        "\n",
        "### Due: February 5 (Wednesday) by 11:59pm\n",
        "\n",
        "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine + explore some LLM capabilities.\n",
        "\n",
        "*Submission instructions (Canvas):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via Canvas (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
        "\n",
        "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
        "\n",
        "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Canvas, search StackOverflow, even use ChatGPT. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. See the course syllabus for details.\n",
        "\n",
        "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
        "\n",
        "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CKllFl-N-Jk"
      },
      "source": [
        "# Dataset: Enron Email Dataset\n",
        "\n",
        "We are providing you with a small collection of emails from the Enron Email Dataset. The Enron Email Dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron. The full corpus contains a total of about 0.5M messages (https://www.cs.cmu.edu/~enron/). For this homework, we will use a small subset of the data. The subset contains 814 emails extracted from the `_sent_mail` of Arnold-J. We have zipped the 814 files (each file contains the information of an email). The zipped file is available on Canvas as `enron_814.zip`. The subset we provide is about 1.1MB. You should treat each email as a unique document to be indexed by your system. You can download the data from Canvas to your local filesystem. We're going to use these emails as the basis of OASIS, our Open Access Searchable Information System!\n",
        "\n",
        "\n",
        "Below is an example of one email.\n",
        "\n",
        "```text\n",
        "Message-ID: <33025919.1075857594206.JavaMail.evans@thyme>\n",
        "Date: Wed, 13 Dec 2000 13:09:00 -0800 (PST)\n",
        "From: john.arnold@enron.com\n",
        "To: slafontaine@globalp.com\n",
        "Subject: re:spreads\n",
        "Mime-Version: 1.0\n",
        "Content-Type: text/plain; charset=us-ascii\n",
        "Content-Transfer-Encoding: 7bit\n",
        "X-From: John Arnold\n",
        "X-To: slafontaine@globalp.com @ ENRON\n",
        "X-cc:\n",
        "X-bcc:\n",
        "X-Folder: \\John_Arnold_Dec2000\\Notes Folders\\'sent mail\n",
        "X-Origin: Arnold-J\n",
        "X-FileName: Jarnold.nsf\n",
        "\n",
        "saw a lot of the bulls sell summer against length in front to mitigate\n",
        "margins/absolute position limits/var.  as these guys are taking off the\n",
        "front, they are also buying back summer.  el paso large buyer of next winter\n",
        "today taking off spreads.  certainly a reason why the spreads were so strong\n",
        "on the way up and such a piece now.   really the only one left with any risk\n",
        "premium built in is h/j now.   it was trading equivalent of 180 on access,\n",
        "down 40+ from this morning.  certainly if we are entering a period of bearish\n",
        "to neutral trade, h/j will get whacked.  certainly understand the arguments\n",
        "for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was\n",
        "trading for 55 on monday.  today it was 10/17.  the market's view of\n",
        "probability of h going crazy has certainly changed in past 48 hours and that\n",
        "has to be reflected in h/j.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "slafontaine@globalp.com on 12/13/2000 04:15:51 PM\n",
        "To: slafontaine@globalp.com\n",
        "cc: John.Arnold@enron.com\n",
        "Subject: re:spreads\n",
        "\n",
        "\n",
        "\n",
        "mkt getting a little more bearish the back of winter i think-if we get another\n",
        "cold blast jan/feb mite move out. with oil moving down and march closer flat\n",
        "px\n",
        "wide to jan im not so bearish these sprds now-less bullish march april as\n",
        "well.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaiOjFANN-Jo"
      },
      "source": [
        "# Part 1: Read and Parse the Email Data (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoiNbqQDrVQ2"
      },
      "source": [
        "Recall how we handled file input in Homework 0? Well, here, our goal is to read the emails so that we can begin to tokenize them later. For this step, you should read the dataset and print the emails. Note that our dataset contains multiple files. You will need to write Python code to read from these files, and then build a list to store the documents. Each item in the list should be a dictionary containing the `Document-ID` as the key, and email content as the value. You can discard all the supplementary information of the email, e.g., `Date`, `From`, `To`, `Subject`, etc.\n",
        "\n",
        "A document should look like:\n",
        "\n",
        "```text\n",
        "{'Document-ID': '33025919.1075857594206',\n",
        "'content': 'saw a lot of the bulls sell summer against length in front to mitigate\n",
        "margins/absolute position limits/var.  as these guys are taking off the\n",
        "front, they are also buying back summer.  el paso large buyer of next winter\n",
        "today taking off spreads.  certainly a reason why the spreads were so strong\n",
        "on the way up and such a piece now.   really the only one left with any risk\n",
        "premium built in is h/j now.   it was trading equivalent of 180 on access,\n",
        "down 40+ from this morning.  certainly if we are entering a period of bearish\n",
        "to neutral trade, h/j will get whacked.  certainly understand the arguments\n",
        "for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was\n",
        "trading for 55 on monday.  today it was 10/17.  the market's view of\n",
        "probability of h going crazy has certainly changed in past 48 hours and that\n",
        "has to be reflected in h/j.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "slafontaine@globalp.com on 12/13/2000 04:15:51 PM\n",
        "To: slafontaine@globalp.com\n",
        "cc: John.Arnold@enron.com\n",
        "Subject: re:spreads\n",
        "\n",
        "\n",
        "\n",
        "mkt getting a little more bearish the back of winter i think-if we get another\n",
        "cold blast jan/feb mite move out. with oil moving down and march closer flat\n",
        "px\n",
        "wide to jan im not so bearish these sprds now-less bullish march april as\n",
        "well.'\n",
        "}\n",
        "```\n",
        "\n",
        "For this homework, you should treat the email content as a document and the Message-ID as the document ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0sMQsv-mg1l"
      },
      "source": [
        "## Print the first two documents (5 points)\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "DocumentID Document\n",
        "\n",
        "33025919.1075857594206 saw a lot of the bulls sell summer ......\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install p7zip-full\n",
        "!7z x enron_814.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uT7laJJodj8",
        "outputId": "affd1923-3d30-4c52-fa07-80fff65914a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 1036170 bytes (1012 KiB)\n",
            "\n",
            "Extracting archive: enron_814.zip\n",
            "--\n",
            "Path = enron_814.zip\n",
            "Type = zip\n",
            "Physical Size = 1036170\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b 54% 933 - __MACOSX/enron_814/._688.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 1468 - enron_814/548.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Folders: 1\n",
            "Files: 1629\n",
            "Size:       1334204\n",
            "Compressed: 1036170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IfHGvybCysQK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# please print out the first 2 docs\n",
        "import re, os, json\n",
        "from itertools import islice\n",
        "Mails=[]\n",
        "doc={}\n",
        "i=1\n",
        "for file in os.listdir(\"enron_814\"):\n",
        "  # print(file)\n",
        "\n",
        "  with open(os.path.join(\"enron_814\",file), 'r', encoding='utf-8') as f:\n",
        "    mail= f.read()\n",
        "\n",
        "    match = re.search(r\"(\\d+\\.\\d+)\", mail)\n",
        "    content_match=re.search(r\"(\\r?\\n){2,}([\\s\\S]+)\", mail)\n",
        "    if match:\n",
        "      document_id = match.group(1)\n",
        "      content=content_match.group(2)\n",
        "      doc[document_id]=content\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-juuk1zLuw"
      },
      "source": [
        "Now that you can read the documents, let's move on to tokenization. We are going to simplify things for you:\n",
        "1. You should **lowercase** all words.\n",
        "2. Replace line breaks (e.g., \\n, \\n\\n), punctuations, dashes and splash (e.g., -, /) and special characters (\\u2019, \\u2005, etc.) with empty space (\" \").\n",
        "3. Tokenize the documents by splitting on whitespace.\n",
        "4. Then only keep words that have a-zA-Z in them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_CpG2ifejjvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc6843b-b98c-467f-fac4-ba6a8d7ebc89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9410\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'[\\n\\r\\t.:]', ' ', text)\n",
        "    text=re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
        "    return tokens\n",
        "\n",
        "tokens=[]\n",
        "for id,mail in doc.items():\n",
        "  tks=custom_tokenize(mail)\n",
        "  for tk in tks:\n",
        "    tokens.append(tk)\n",
        "print(len(set(tokens)))\n",
        "lookup_doc=doc.copy()\n",
        "# print(doc)\n",
        "# print(lookup_doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tst9U9DxN-Jp"
      },
      "source": [
        "## Print the first two documents after tokenizing (5 points)\n",
        "\n",
        "Once you have your parser working, you should print the first two documents (documentID and tokens).\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "* DocumentID Tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WcJbBEQgJUd8"
      },
      "outputs": [],
      "source": [
        "# your code and output here\n",
        "for id,mail in doc.items():\n",
        "  doc[id]=custom_tokenize(mail)\n",
        "# print(lookup_doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for key, value in islice(doc.items(), 2):\n",
        "    print(key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXqQ6uL2ZysM",
        "outputId": "51cdbea3-fb25-4ca7-f599-aa1b901144d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20191135.1075857601260 ['hey', 'can', 'greg', 'fit', 'me', 'in', 'for', 'about', 'minutes', 'tomorrow', 'afternoon', 'your', 'secret', 'admirer']\n",
            "26241159.1075857650422 ['how', 'about', 'we', 'celebrate', 'the', 'near', 'completion', 'of', 'your', 'deal', 'and', 'have', 'tickleless', 'pay', 'for', 'it', 'from', 'kim', 'ward', 'enron', 'enronxgate', 'on', 'pm', 'to', 'john', 'arnold', 'hou', 'ect', 'ect', 'cc', 'subject', 're', 'push', 'the', 'summer', 'down', 'about', 'so', 'i', 'can', 'get', 'one', 'of', 'my', 'deals', 'done', 'and', 'we', 'could', 'celebrate', 'original', 'message', 'from', 'arnold', 'john', 'sent', 'tuesday', 'may', 'pm', 'to', 'ward', 'kim', 's', 'subject', 'wanna', 'get', 'sauced', 'after', 'work']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2yVi76Ij1ys"
      },
      "source": [
        "## Dictionary Size (5 points)\n",
        "\n",
        "Next you should report the size of your dictionary, that is, how many unique tokens among all the documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "53bAA66zkI55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bacd5bdd-ef8d-4fa6-978b-d846adca1498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of Unique Tokens: 9410\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127149"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(\"No.of Unique Tokens:\",len(set(tokens)))\n",
        "len(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU928IKKkJGY"
      },
      "source": [
        "## Top-20 Words (5 points)\n",
        "\n",
        "Finally, you should print a list of the top-20 most popular words by counting among all documents.\n",
        "\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "* Rank. Token, Count\n",
        "\n",
        "1. awesome, 20\n",
        "2. cool, 15\n",
        "3. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8MS6n6oNklvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1bfb25de-2f9d-4d82-aba3-e10180d28532"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 4111),\n",
              " ('to', 3768),\n",
              " ('ect', 2600),\n",
              " ('enron', 2296),\n",
              " ('a', 1897),\n",
              " ('and', 1795),\n",
              " ('you', 1749),\n",
              " ('of', 1716),\n",
              " ('i', 1696),\n",
              " ('on', 1590),\n",
              " ('john', 1514),\n",
              " ('in', 1465),\n",
              " ('hou', 1345),\n",
              " ('com', 1319),\n",
              " ('is', 1202),\n",
              " ('for', 1165),\n",
              " ('arnold', 999),\n",
              " ('subject', 936),\n",
              " ('s', 871),\n",
              " ('it', 865)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter(tokens)\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_word_counts[:20]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcrFD2F-N-Jp"
      },
      "source": [
        "# Part 2: Boolean Retrieval (30 points)\n",
        "\n",
        "In this part you will build an inverted index to support Boolean retrieval. You should use the tokenization strategy from above.\n",
        "\n",
        "We require your index to support AND, OR, NOT queries.\n",
        "\n",
        "Search for the queries below using your index and print out matching documents (for each query, print out 5 matching documents):\n",
        "* buyer\n",
        "* margins AND limits\n",
        "* winter OR summer\n",
        "* buyers AND risk AND NOT crazy\n",
        "* never OR know\n",
        "\n",
        "Recall, that you should apply the exact same pre-processing strategies to the query as we do to the documents.\n",
        "\n",
        "The output should like this:\n",
        "* DocumentID Document\n",
        "\n",
        "To make our life easier, please output the DocumentIDs in lexicographic order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "GS3Tc_kYN-Jp"
      },
      "outputs": [],
      "source": [
        "# build the index here\n",
        "# add cells as needed to organize your code\n",
        "\n",
        "##Building inverted Index\n",
        "inv_index={}\n",
        "for key, tokens in doc.items():\n",
        "  for token in tokens:\n",
        "    if token not in inv_index:\n",
        "      inv_index[token]=[]\n",
        "    inv_index[token].append(key)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def retrieve_docs_only(query):\n",
        "#   query=query.split()\n",
        "#   print(query)\n",
        "#   retrieved_docs=[]\n",
        "#   for i in range(len(query)):\n",
        "#     # print(retrieved_docs)\n",
        "#     if i%2==0:\n",
        "#       if query[i] not in inv_index:\n",
        "#         return []\n",
        "#       else:\n",
        "#         retrieved_docs.append(inv_index[query[i]])\n",
        "#     # elif i%2==1:\n",
        "#     if query[i]=='NOT':\n",
        "#       retrieved_docs.append(list(set(doc.keys())-set(inv_index[query[i]])))\n",
        "#       del query[i]\n",
        "#       print(query)\n",
        "#   return retrieved_docs\n",
        "# def perform_operations(query, retrieved_docs):\n",
        "#   if type(query)== str:\n",
        "#     query=query.split()\n",
        "#   if len(query)<=1:\n",
        "#     return retrieved_docs\n",
        "#   # print(query)\n",
        "#   if query[1]== 'OR':\n",
        "#     retrieved_docs[0]=set(retrieved_docs[0]+retrieved_docs[1])\n",
        "#   elif query[1]== 'AND':\n",
        "#     # print(retrieved_docs)\n",
        "#     retrieved_docs[0]=list(set(retrieved_docs[0]).intersection(retrieved_docs[1]))\n",
        "#   # elif query[1]== 'NOT':\n",
        "#   #   retrieved_docs[0]=list(set(retrieved_docs[0])-set(retrieved_docs[1]))\n",
        "#   # return retrieved_docs[0]\n",
        "#   if len(retrieved_docs) >= 2:del retrieved_docs[1]\n",
        "#   del query[:3]\n",
        "#   # print(retrieved_docs)\n",
        "#   return perform_operations(query, retrieved_docs)\n",
        "# query=\"buyers AND risk AND NOT crazy\"\n",
        "# retrieved_docs=retrieve_docs_only(query)\n",
        "# # print(retrieved_docs)\n",
        "# r_doc_id = list(perform_operations(query, retrieved_docs)[0])\n",
        "# print(r_doc_id)\n",
        "# for doc_id in r_doc_id:\n",
        "#   print(doc_id, lookup_doc[doc_id])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MOnEDvalb7al"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(lookup_doc)"
      ],
      "metadata": {
        "id": "sB-keUMS6vkc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82YUVPUknOjX"
      },
      "source": [
        "## Running the five queries (4 points each, 20 points in total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "qWH8h5ZkN-Jp"
      },
      "outputs": [],
      "source": [
        "def preprocess_query(query):\n",
        "    return query.lower().split()\n",
        "\n",
        "def and_query(inv_index, terms):\n",
        "    if not terms:\n",
        "        return []\n",
        "    processed_result = set(inv_index.get(terms[0], []))\n",
        "    for term in terms[1:]:\n",
        "        processed_result &= set(inv_index.get(term, []))\n",
        "    return list(processed_result)\n",
        "\n",
        "def or_query(inv_index, terms):\n",
        "    processed_result = set()\n",
        "    for term in terms:\n",
        "        processed_result |= set(inv_index.get(term, []))\n",
        "    return list(processed_result)\n",
        "\n",
        "def not_query(inv_index, term, all_docs):\n",
        "    docs_with_term = set(inv_index.get(term, []))\n",
        "    return list(all_docs - docs_with_term)\n",
        "\n",
        "def parse_query(inv_index, query, all_docs):\n",
        "    terms = preprocess_query(query)\n",
        "    if not terms:\n",
        "        return list(all_docs)\n",
        "    i = 0\n",
        "    processed_terms = []\n",
        "    while i < len(terms):\n",
        "        if terms[i] == \"not\" and i + 1 < len(terms):\n",
        "            not_processed_result = set(not_query(inv_index, terms[i + 1], all_docs))\n",
        "            processed_terms.append((\"TERM\", not_processed_result))\n",
        "            i += 2\n",
        "        else:\n",
        "            if terms[i] not in (\"and\", \"or\"):\n",
        "                term_docs = set(inv_index.get(terms[i], []))\n",
        "                processed_terms.append((\"TERM\", term_docs))\n",
        "            else:\n",
        "                processed_terms.append((\"OP\", terms[i]))\n",
        "            i += 1\n",
        "    i = 0\n",
        "    and_processed = []\n",
        "    while i < len(processed_terms):\n",
        "        if i == 0 and processed_terms[i][0] != \"OP\":\n",
        "            and_processed.append((\"TERM\", processed_terms[i][1]))\n",
        "            i += 1\n",
        "        elif i+1 < len(terms) and processed_terms[i][1] == \"and\":\n",
        "            left_type, left_docs   = and_processed[-1]\n",
        "            right_type, right_docs = processed_terms[i+1]\n",
        "            if left_type in (\"TERM\") and right_type in (\"TERM\"):\n",
        "                and_processed_result        = left_docs & right_docs\n",
        "                and_processed[-1] = (\"TERM\", and_processed_result)\n",
        "                i += 2\n",
        "            else:\n",
        "                and_processed.append(processed_terms[i])\n",
        "                i += 1\n",
        "        else:\n",
        "            and_processed.append(processed_terms[i])\n",
        "            i += 1\n",
        "    i = 0\n",
        "    final_processed = []\n",
        "    while i < len(and_processed):\n",
        "        if i == 0 and and_processed[i][0] != \"OP\":\n",
        "            final_processed.append((\"TERM\", and_processed[i][1]))\n",
        "            i += 1\n",
        "        elif i+1 < len(terms) and and_processed[i][1] == \"or\":\n",
        "            left_type, left_docs   = final_processed[-1]\n",
        "            right_type, right_docs = and_processed[i+1]\n",
        "            if left_type in (\"TERM\") and right_type in (\"TERM\"):\n",
        "                or_processed_result           = left_docs | right_docs\n",
        "                final_processed[-1] = (\"TERM\", or_processed_result)\n",
        "                i += 2\n",
        "            else:\n",
        "                final_processed.append(and_processed[i])\n",
        "                i += 1\n",
        "        else:\n",
        "            final_processed.append(and_processed[i])\n",
        "            i += 1\n",
        "    return list(set(final_processed[-1][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzx6tpmpjBsX"
      },
      "source": [
        "Now show the results for the query: `buyer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gUOUjF18jFTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "f2cf0f27-1fe1-47dc-d5a8-bbcff3a7e145"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['28376645.1075857655238',\n",
              " '25732708.1075857656969',\n",
              " '5307647.1075857657213',\n",
              " '17195279.1075857655281',\n",
              " '3677051.1075857657147',\n",
              " '33025919.1075857594206',\n",
              " '13960264.1075857658698',\n",
              " '2268604.1075857652949',\n",
              " '5304428.1075857597649']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"buyer\"\n",
        "parse_query(inv_index, query, set(doc.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbJ6yn3yMQWY"
      },
      "source": [
        "*Now* show the results for the query: `margins AND limits`\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SmF62rQ_MRlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8dc21622-6e6e-43b5-95e9-3935d770a14f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['33025919.1075857594206']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"margins AND limits\"\n",
        "parse_query(inv_index, query, set(doc.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KlaH0fMTO5"
      },
      "source": [
        "Now show the results for the query: `winter OR summer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MAFBtPGmMVRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c40a9508-2398-4c90-d0be-e87942bdbb6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['31307130.1075857655193',\n",
              " '26583745.1075857594731',\n",
              " '2363438.1075857657841',\n",
              " '12760800.1075857651684',\n",
              " '26173217.1075857594537',\n",
              " '33491127.1075857594966',\n",
              " '5307647.1075857657213',\n",
              " '1475999.1075857651728',\n",
              " '33358932.1075857653571',\n",
              " '13960264.1075857658698',\n",
              " '18520811.1075857650486',\n",
              " '17805507.1075857658940',\n",
              " '18731712.1075857658964',\n",
              " '11971129.1075857654956',\n",
              " '2268604.1075857652949',\n",
              " '19675930.1075857656429',\n",
              " '30030513.1075857650336',\n",
              " '3126127.1075857653305',\n",
              " '14369391.1075857597800',\n",
              " '13876367.1075857655500',\n",
              " '23667211.1075857656689',\n",
              " '24632457.1075857653283',\n",
              " '29406139.1075857594335',\n",
              " '6398802.1075857654979',\n",
              " '15042778.1075857600714',\n",
              " '10353423.1075857652669',\n",
              " '33025919.1075857594206',\n",
              " '19850187.1075857654782',\n",
              " '14371860.1075857601017',\n",
              " '10603457.1075857597605',\n",
              " '32415330.1075857655843',\n",
              " '773676.1075857654425',\n",
              " '19235579.1075857594400',\n",
              " '17721019.1075857596305',\n",
              " '23931433.1075857654804',\n",
              " '30621564.1075857650358',\n",
              " '27427045.1075857651911',\n",
              " '18032638.1075857655456',\n",
              " '29844582.1075857650401',\n",
              " '15023476.1075857654287',\n",
              " '12541953.1075857657123',\n",
              " '26239621.1075857651225',\n",
              " '6009358.1075857654847',\n",
              " '455652.1075857654869',\n",
              " '22562856.1075857596586',\n",
              " '32310419.1075857650380',\n",
              " '10537445.1075857655390',\n",
              " '5850542.1075857652496',\n",
              " '33042577.1075857658502',\n",
              " '2340358.1075857651846',\n",
              " '26241159.1075857650422',\n",
              " '7181067.1075857602318',\n",
              " '2275103.1075857654157',\n",
              " '1464788.1075857651573',\n",
              " '21723237.1075857656903',\n",
              " '21884118.1075857658063',\n",
              " '3677051.1075857657147',\n",
              " '5508781.1075857658744']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"winter OR summer\"\n",
        "parse_query(inv_index, query, set(doc.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVcjjsBWIFv"
      },
      "source": [
        "Now show the results for the query: `buyers AND risk AND NOT crazy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3e7Ipk2JWIF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790665e6-ae9f-404e-cb91-6f7c3cb61803"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2726985.1075857655016']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"buyers AND risk AND NOT crazy\"\n",
        "parse_query(inv_index, query, set(doc.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ0sd8zrWIbb"
      },
      "source": [
        "Now show the results for the query: `never OR know`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g2BumYswWIbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5183eacd-f337-4741-890b-11f97a0504ee",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['31307130.1075857655193',\n",
              " '26813632.1075857653239',\n",
              " '6099369.1075857598441',\n",
              " '6384662.1075857656041',\n",
              " '8618376.1075857651868',\n",
              " '22083600.1075857596413',\n",
              " '19835539.1075857596349',\n",
              " '18520811.1075857650486',\n",
              " '2268604.1075857652949',\n",
              " '18731712.1075857658964',\n",
              " '7293239.1075857597953',\n",
              " '3574764.1075857600475',\n",
              " '3126127.1075857653305',\n",
              " '23057737.1075857601038',\n",
              " '13876367.1075857655500',\n",
              " '17259122.1075857598177',\n",
              " '3073341.1075857596866',\n",
              " '18917089.1075857653085',\n",
              " '20679245.1075857597281',\n",
              " '10353423.1075857652669',\n",
              " '27187948.1075857650684',\n",
              " '26270100.1075857598045',\n",
              " '10396784.1075857651750',\n",
              " '32776644.1075857658676',\n",
              " '773676.1075857654425',\n",
              " '25124831.1075857599419',\n",
              " '2200629.1075857599483',\n",
              " '7410631.1075857598982',\n",
              " '10008095.1075857595829',\n",
              " '28376645.1075857655238',\n",
              " '18346325.1075857654266',\n",
              " '17569138.1075857658721',\n",
              " '17195279.1075857655281',\n",
              " '23707366.1075857595937',\n",
              " '13443589.1075857600218',\n",
              " '1837490.1075857597060',\n",
              " '21351833.1075857600305',\n",
              " '1654017.1075857597017',\n",
              " '5446449.1075857594451',\n",
              " '22562856.1075857596586',\n",
              " '12628033.1075857597082',\n",
              " '14535951.1075857651999',\n",
              " '3411593.1075857653960',\n",
              " '1460363.1075857657366',\n",
              " '11976444.1075857651507',\n",
              " '32764884.1075857601388',\n",
              " '8100668.1075857597150',\n",
              " '21808189.1075857649876',\n",
              " '33309159.1075857595979',\n",
              " '27366520.1075857596930',\n",
              " '7442783.1075857594944',\n",
              " '2363438.1075857657841',\n",
              " '30651679.1075857655040',\n",
              " '28367005.1075857650509',\n",
              " '7736085.1075857654891',\n",
              " '3320827.1075857598463',\n",
              " '11971129.1075857654956',\n",
              " '32892980.1075857598002',\n",
              " '9855848.1075857597735',\n",
              " '14965071.1075857599113',\n",
              " '30031046.1075857598528',\n",
              " '19333123.1075857650073',\n",
              " '17611484.1075857597172',\n",
              " '29406139.1075857594335',\n",
              " '1344885.1075857600283',\n",
              " '6398802.1075857654979',\n",
              " '27868892.1075857596716',\n",
              " '15042778.1075857600714',\n",
              " '18076120.1075857599934',\n",
              " '17579414.1075857597367',\n",
              " '14055809.1075857655996',\n",
              " '29233255.1075857650051',\n",
              " '17721019.1075857596305',\n",
              " '18032638.1075857655456',\n",
              " '14858888.1075857651793',\n",
              " '16267167.1075857654394',\n",
              " '2039389.1075857653810',\n",
              " '24895032.1075857600736',\n",
              " '1235794.1075857653547',\n",
              " '26239621.1075857651225',\n",
              " '25282344.1075857601734',\n",
              " '27677135.1075857653981',\n",
              " '1075367.1075857649810',\n",
              " '9513095.1075857597540',\n",
              " '20171259.1075857653481',\n",
              " '27416256.1075857657862',\n",
              " '7584611.1075857597105',\n",
              " '20430944.1075857659032',\n",
              " '12569154.1075857597193',\n",
              " '17042690.1075857650186',\n",
              " '18627553.1075857661524',\n",
              " '18120216.1075857657080',\n",
              " '10337786.1075857649963',\n",
              " '27343688.1075863711046',\n",
              " '23610249.1075857595011',\n",
              " '13960264.1075857658698',\n",
              " '17805507.1075857658940',\n",
              " '21218946.1075857649940',\n",
              " '16621332.1075857602475',\n",
              " '15285068.1075857653938',\n",
              " '17767559.1075857594623',\n",
              " '10216181.1075857658324',\n",
              " '13844738.1075857596392',\n",
              " '71656.1075857599569',\n",
              " '25732708.1075857656969',\n",
              " '6552660.1075857656991',\n",
              " '24632457.1075857653283',\n",
              " '17347344.1075857657819',\n",
              " '2726985.1075857655016',\n",
              " '14371860.1075857601017',\n",
              " '19955161.1075857597714',\n",
              " '15158297.1075857650899',\n",
              " '26144738.1075857594796',\n",
              " '32239363.1075857595807',\n",
              " '8779306.1075857599091',\n",
              " '16208484.1075857658986',\n",
              " '8497228.1075857599048',\n",
              " '32730441.1075857653351',\n",
              " '27533790.1075857650727',\n",
              " '6872234.1075857597932',\n",
              " '31328170.1075857655909',\n",
              " '23784307.1075857597844',\n",
              " '11245962.1075857650748',\n",
              " '16040614.1075857657388',\n",
              " '12785682.1075857656018',\n",
              " '10537445.1075857655390',\n",
              " '10741607.1075857600950',\n",
              " '21884118.1075857658063',\n",
              " '29352074.1075857650639',\n",
              " '19407296.1075857650617',\n",
              " '1280210.1075857597127',\n",
              " '3677051.1075857657147',\n",
              " '30146890.1075857597910',\n",
              " '24512879.1075857657431',\n",
              " '18518186.1075857596951',\n",
              " '20827910.1075857651772',\n",
              " '8614134.1075857655125',\n",
              " '27496330.1075857599135',\n",
              " '26702401.1075857595677',\n",
              " '11476753.1075857653064',\n",
              " '19333648.1075857594666',\n",
              " '6935248.1075857594516',\n",
              " '22399752.1075857653438',\n",
              " '30359418.1075857599376',\n",
              " '11256811.1075857601060',\n",
              " '32362043.1075857600064',\n",
              " '23822038.1075857658431',\n",
              " '438850.1075863711023',\n",
              " '12123988.1075857598485',\n",
              " '25081271.1075857600086',\n",
              " '28221841.1075857594558',\n",
              " '30235823.1075857656925',\n",
              " '23299574.1075857651595',\n",
              " '4400718.1075857598088',\n",
              " '10618229.1075857657058',\n",
              " '4555533.1075857596131',\n",
              " '15299653.1075857662533',\n",
              " '3181514.1075857658873',\n",
              " '5296469.1075857658367',\n",
              " '12941463.1075857599243',\n",
              " '8912200.1075857598110',\n",
              " '7181067.1075857602318',\n",
              " '21362877.1075857652540',\n",
              " '23808285.1075857598702',\n",
              " '10480477.1075857649616',\n",
              " '32586900.1075857594580',\n",
              " '10825199.1075857602190',\n",
              " '4608532.1075857651485']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"never OR know\"\n",
        "parse_query(inv_index, query, set(doc.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAhHuIDfN-Jp"
      },
      "source": [
        "## Observations (10 points)\n",
        "Does your boolean search engine find relevant documents for these queries? As in, would our customers be happy if we shipped this retrieval engine? Why or why not?\n",
        "\n",
        "What is the impact of the pre-processing options? Do they impact your search quality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX6k-Vil0GIy"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "#### The search engine is fetching relevant documents for the queries. The customers will be happy as it makes their search easy and the search engine has the capability for long queries. So, the user can modify their search for exact results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5JYoNNrlimy"
      },
      "source": [
        "# Part 3: Ranking (35 points)\n",
        "\n",
        "For the third part, you will add ranking to your search system. Given a search query, our goal is to retrieve the top-5 most relevant emails by assigning a score to each document.\n",
        "\n",
        "We will explore two ranking methods, each offering a different approach to scoring and ranking documents:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMlmJ-8mP8X"
      },
      "source": [
        "### Ranking method A: Ranking with vector space model with TF-IDF (15 points)\n",
        "\n",
        "**Cosine:** You should use cosine as your scoring function.\n",
        "\n",
        "**TFIDF:** For the **document vectors**, use the standard TF-IDF scores introduced in class. For the **query vector**, use **simple weights (the raw term frequency)**. For example:\n",
        "* query: never $\\rightarrow$ (1)\n",
        "* query: never know $\\rightarrow$ (1, 1)\n",
        "\n",
        "**Query:**  `trade`\n",
        "\n",
        "**Output:**\n",
        "You should output the top-5 results plus the cosine score of each of these documents.  \n",
        "\n",
        "The output should be like this:\n",
        "\n",
        "Rank Scores DocumentID Document\n",
        "\n",
        "---\n",
        "\n",
        "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3OMOP3bomAM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6399d998-65ef-4c42-de4f-1d75a0be77d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'trade': 1}\n",
            "Rank: 1 cosine_score: 0.35268279961678006 document_id: 32835197.1075857597302 \n",
            " Hey:\n",
            "I just want to confirm the trades I have in your book.\n",
            "Trade #1.  I sell 4000 X @ 4652\n",
            "\n",
            "Trade #2. I buy 4000 X @ 4652\n",
            "  I sell 4000 X @ 4902\n",
            "\n",
            "Trade #3 I buy 4000 X @ 5000\n",
            "  I sell 4000 F @ 5000\n",
            "\n",
            "\n",
            "Net result: I have 4000 F in your book @ 4902.\n",
            "Thanks, \n",
            "John\n",
            "\n",
            "-------------------------------------------------\n",
            "Rank: 2 cosine_score: 0.3053426239418323 document_id: 15827855.1075857658654 \n",
            " torrey:\n",
            "please set me up to trade crude.\n",
            "John\n",
            "\n",
            "-------------------------------------------------\n",
            "Rank: 3 cosine_score: 0.2283779396714838 document_id: 2752057.1075857658632 \n",
            " Torrey:\n",
            "Can you also approve Mike Maggi to trade crude as well.  Thanks for your help.\n",
            "John\n",
            "\n",
            "-------------------------------------------------\n",
            "Rank: 4 cosine_score: 0.22160785591147464 document_id: 3383202.1075857656796 \n",
            " you fucker that's my trade.   i was trying to buy nines the last 20 minutes.  \n",
            "all i got was scraps.  50-100.  i think it's a great trade.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slafontaine@globalp.com on 02/07/2001 01:41:44 PM\n",
            "To: John.Arnold@enron.com\n",
            "cc:  \n",
            "Subject: Re: weather pop\n",
            "\n",
            "\n",
            "\n",
            "that is nuts-good sale-im gonna sell jun or july otm calls at some point\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------\n",
            "Rank: 5 cosine_score: 0.1852881258960075 document_id: 5340834.1075857658345 \n",
            " greg:\n",
            "what is the (correct) formula you devised for profitability on last trade is \n",
            "mid?\n",
            "\n",
            "-------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# print(word_counts)\n",
        "from collections import defaultdict\n",
        "import math\n",
        "# for key, value in islice(doc.items(), 2):\n",
        "#     print(key, value)\n",
        "\n",
        "def tfidf(doc):\n",
        "  doc_feq=defaultdict(int)\n",
        "  tfidf_scores={}\n",
        "  for tks in doc.values():\n",
        "    for token in set(tks):\n",
        "      doc_feq[token]+=1\n",
        "  for doc_id, tks in doc.items():\n",
        "      tfidf_vector={}\n",
        "      total_tks=len(tks)\n",
        "      for t in tks:\n",
        "        tf=tks.count(t)/total_tks\n",
        "        idf=math.log(len(doc)/doc_feq[t])\n",
        "        tfidf_vector[t]=tf*idf\n",
        "      # doc[doc_id]=tfidf_vector\n",
        "      tfidf_scores[doc_id]=tfidf_vector\n",
        "  return tfidf_scores\n",
        "\n",
        "def cosine_similarity(query_vector, doc_vector):\n",
        "    common_keys=set(query_vector.keys()).intersection(set(doc_vector.keys()))\n",
        "    # for key in common_keys:\n",
        "    dot_product=sum(query_vector[key]*doc_vector[key] for key in common_keys)\n",
        "    query_magnitude=math.sqrt(sum(query_vector[key]**2 for key in query_vector))\n",
        "    doc_magnitude=math.sqrt(sum(doc_vector[key]**2 for key in doc_vector))\n",
        "    if query_magnitude==0 or doc_magnitude==0:\n",
        "      return 0\n",
        "    return dot_product/(query_magnitude*doc_magnitude)\n",
        "\n",
        "tfidf_scores=tfidf(doc)\n",
        "# print(tfidf_scores)\n",
        "query=\"trade\"\n",
        "query_vector={}\n",
        "query_token=custom_tokenize(query)\n",
        "for key in query_token:\n",
        "  query_vector[key]=1\n",
        "print(query_vector)\n",
        "scores={}\n",
        "for doc_id, doc_vector in tfidf_scores.items():\n",
        "  scores[doc_id]=cosine_similarity(query_vector, doc_vector)\n",
        "ranked_results=sorted(scores.items(), key=lambda x: (x[1],x[0]), reverse=True)[:5]\n",
        "# print(ranked_results)\n",
        "for i in range(len(ranked_results)):\n",
        "  print(\"Rank:\",i+1, \"cosine_score:\",ranked_results[i][1], \"document_id:\",ranked_results[i][0], \"\\n\", lookup_doc[ranked_results[i][0]])\n",
        "  print(\"\\n-------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print out the top-5 retrieved emails\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf3-HsRamBkh"
      },
      "source": [
        "### Ranking method B: Ranking with BM25 (15 points)\n",
        "Finally, let's try the BM25 approach for ranking. Refer to https://en.wikipedia.org/wiki/Okapi_BM25 for the specific formula. You could choose k_1 = 1.2 and b = 0.75 but feel free to try other options.\n",
        "\n",
        "**Query:**  `gas floor`\n",
        "\n",
        "**Output:**\n",
        "You should output the top-5 results plus the BM25 score of each of these documents.  \n",
        "\n",
        "The output should be like this:\n",
        "\n",
        "Rank Scores DocumentID Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lESwxZNImIle"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "\n",
        "# print out the top-5 retrieved emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dScZI3PvnNqV"
      },
      "source": [
        "## Observations (5 points)\n",
        "What do you observe? Are there key differences between the two ranking approaches? Briefly discuss in bullet points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gku0X4oFrBRX"
      },
      "source": [
        "* Your observations:\n",
        "* Differences:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwQKelDxgzS8"
      },
      "source": [
        "# Part 4: Cool LLM RAG Extension (15 points)\n",
        "\n",
        "Finally, we give you an opportunity to explore using OASIS for Retrieval-Augmented Generation (RAG) with LLMs.\n",
        "Here, the task is to retrieve the top-5 emails for a query you like. You will then pass the retrieved email content along with your question to the LLM and let it answer your question. Specifically, we want you:\n",
        "\n",
        "* Query the LLM directly with your question.\n",
        "* Retrieve the top-5 emails based on your query and pass them to the LLM along with your question.\n",
        "* How is the RAG output different from the non-RAG output? Does retrieval help the LLM better answer your question?\n",
        "\n",
        "We recommend using Gemini following the [instructions](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb) here.\n",
        "\n",
        "Hint: Take a close look at the dataset and pick a specific, relevant query where retrieval can enhance the LLMs response.\n",
        "\n",
        "*What You Will Submit*\n",
        "- Your query.\n",
        "- The top-5 retrieved emails (including Document ID and ranking score).\n",
        "- The LLM's response without retrieval.\n",
        "- The LLM's response with retrieval (RAG).\n",
        "- A brief analysis comparing both outputs.\n",
        "\n",
        "We will grade this last part according to correctness, effort, and creativity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irxiT-iTgzS9"
      },
      "source": [
        "## Step 1: Query the LLM Directly (Without Retrieval) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JiyOf67igzS9"
      },
      "outputs": [],
      "source": [
        "# your code here to prompt LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otHscQFhgzTN"
      },
      "source": [
        "## Step 2: Query the LLM with Retrieved Emails (RAG) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TnArY1PbgzTN"
      },
      "outputs": [],
      "source": [
        "# print out the LLM response without the email content\n",
        "\n",
        "\n",
        "# print out the LLM response with the email content (RAG)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YnqxBZ4gzTN"
      },
      "source": [
        "## Discussion (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyVyTcM-gzTN"
      },
      "source": [
        "In this section, reflect on the performance of different ranking methods and the impact of retrieval on LLM responses. Consider:\n",
        "\n",
        "- How did retrieval affect the LLMs response? Did it improve factual accuracy or relevance?\n",
        "- Were there cases where retrieval hurt performance (e.g., irrelevant documents, redundancy)?\n",
        "- Any ideas for improving the ranking or retrieval process?\n",
        "\n",
        "Keep your discussion *concise* and *insightful*, focusing on key takeaways from your experiments. Please answer in bullet points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMjckcmgzTN"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "* point 1\n",
        "* point 2\n",
        "* ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FihN3qniN-Jy"
      },
      "source": [
        "# Collaboration Declarations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKp_Fe-N-Jy"
      },
      "source": [
        "*You should fill out your collaboration declarations here.*\n",
        "\n",
        "https://stackoverflow.com/questions/12897374/get-unique-values-from-a-list-in-python\n",
        "\n",
        "\n",
        "https://www.geeksforgeeks.org/python-difference-two-lists/\n",
        "\n",
        "ChatGPT:\n",
        "- i want to no. of occurences of each element in a list"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ASjn0YYqcAr"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}