{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsuj1th/Colab/blob/main/33500274_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAxczUGVN-Jc"
      },
      "source": [
        "#### CSCE 670 :: Information Storage & Retrieval :: Texas A&M University :: Spring 2025\n",
        "\n",
        "\n",
        "# Homework 1:  OASIS Search Engine, The Beginning\n",
        "\n",
        "### 100 points [4% of your final grade]\n",
        "\n",
        "### Due: February 5 (Wednesday) by 11:59pm\n",
        "\n",
        "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine + explore some LLM capabilities.\n",
        "\n",
        "*Submission instructions (Canvas):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via Canvas (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
        "\n",
        "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
        "\n",
        "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Canvas, search StackOverflow, even use ChatGPT. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. See the course syllabus for details.\n",
        "\n",
        "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
        "\n",
        "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CKllFl-N-Jk"
      },
      "source": [
        "# Dataset: Enron Email Dataset\n",
        "\n",
        "We are providing you with a small collection of emails from the Enron Email Dataset. The Enron Email Dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron. The full corpus contains a total of about 0.5M messages (https://www.cs.cmu.edu/~enron/). For this homework, we will use a small subset of the data. The subset contains 814 emails extracted from the `_sent_mail` of Arnold-J. We have zipped the 814 files (each file contains the information of an email). The zipped file is available on Canvas as `enron_814.zip`. The subset we provide is about 1.1MB. You should treat each email as a unique document to be indexed by your system. You can download the data from Canvas to your local filesystem. We're going to use these emails as the basis of OASIS, our Open Access Searchable Information System!\n",
        "\n",
        "\n",
        "Below is an example of one email.\n",
        "\n",
        "```text\n",
        "Message-ID: <33025919.1075857594206.JavaMail.evans@thyme>\n",
        "Date: Wed, 13 Dec 2000 13:09:00 -0800 (PST)\n",
        "From: john.arnold@enron.com\n",
        "To: slafontaine@globalp.com\n",
        "Subject: re:spreads\n",
        "Mime-Version: 1.0\n",
        "Content-Type: text/plain; charset=us-ascii\n",
        "Content-Transfer-Encoding: 7bit\n",
        "X-From: John Arnold\n",
        "X-To: slafontaine@globalp.com @ ENRON\n",
        "X-cc:\n",
        "X-bcc:\n",
        "X-Folder: \\John_Arnold_Dec2000\\Notes Folders\\'sent mail\n",
        "X-Origin: Arnold-J\n",
        "X-FileName: Jarnold.nsf\n",
        "\n",
        "saw a lot of the bulls sell summer against length in front to mitigate\n",
        "margins/absolute position limits/var.  as these guys are taking off the\n",
        "front, they are also buying back summer.  el paso large buyer of next winter\n",
        "today taking off spreads.  certainly a reason why the spreads were so strong\n",
        "on the way up and such a piece now.   really the only one left with any risk\n",
        "premium built in is h/j now.   it was trading equivalent of 180 on access,\n",
        "down 40+ from this morning.  certainly if we are entering a period of bearish\n",
        "to neutral trade, h/j will get whacked.  certainly understand the arguments\n",
        "for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was\n",
        "trading for 55 on monday.  today it was 10/17.  the market's view of\n",
        "probability of h going crazy has certainly changed in past 48 hours and that\n",
        "has to be reflected in h/j.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "slafontaine@globalp.com on 12/13/2000 04:15:51 PM\n",
        "To: slafontaine@globalp.com\n",
        "cc: John.Arnold@enron.com\n",
        "Subject: re:spreads\n",
        "\n",
        "\n",
        "\n",
        "mkt getting a little more bearish the back of winter i think-if we get another\n",
        "cold blast jan/feb mite move out. with oil moving down and march closer flat\n",
        "px\n",
        "wide to jan im not so bearish these sprds now-less bullish march april as\n",
        "well.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaiOjFANN-Jo"
      },
      "source": [
        "# Part 1: Read and Parse the Email Data (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoiNbqQDrVQ2"
      },
      "source": [
        "Recall how we handled file input in Homework 0? Well, here, our goal is to read the emails so that we can begin to tokenize them later. For this step, you should read the dataset and print the emails. Note that our dataset contains multiple files. You will need to write Python code to read from these files, and then build a list to store the documents. Each item in the list should be a dictionary containing the `Document-ID` as the key, and email content as the value. You can discard all the supplementary information of the email, e.g., `Date`, `From`, `To`, `Subject`, etc.\n",
        "\n",
        "A document should look like:\n",
        "\n",
        "```text\n",
        "{'Document-ID': '33025919.1075857594206',\n",
        "'content': 'saw a lot of the bulls sell summer against length in front to mitigate\n",
        "margins/absolute position limits/var.  as these guys are taking off the\n",
        "front, they are also buying back summer.  el paso large buyer of next winter\n",
        "today taking off spreads.  certainly a reason why the spreads were so strong\n",
        "on the way up and such a piece now.   really the only one left with any risk\n",
        "premium built in is h/j now.   it was trading equivalent of 180 on access,\n",
        "down 40+ from this morning.  certainly if we are entering a period of bearish\n",
        "to neutral trade, h/j will get whacked.  certainly understand the arguments\n",
        "for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was\n",
        "trading for 55 on monday.  today it was 10/17.  the market's view of\n",
        "probability of h going crazy has certainly changed in past 48 hours and that\n",
        "has to be reflected in h/j.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "slafontaine@globalp.com on 12/13/2000 04:15:51 PM\n",
        "To: slafontaine@globalp.com\n",
        "cc: John.Arnold@enron.com\n",
        "Subject: re:spreads\n",
        "\n",
        "\n",
        "\n",
        "mkt getting a little more bearish the back of winter i think-if we get another\n",
        "cold blast jan/feb mite move out. with oil moving down and march closer flat\n",
        "px\n",
        "wide to jan im not so bearish these sprds now-less bullish march april as\n",
        "well.'\n",
        "}\n",
        "```\n",
        "\n",
        "For this homework, you should treat the email content as a document and the Message-ID as the document ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0sMQsv-mg1l"
      },
      "source": [
        "## Print the first two documents (5 points)\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "DocumentID Document\n",
        "\n",
        "33025919.1075857594206 saw a lot of the bulls sell summer ......\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install p7zip-full\n",
        "!7z x enron_814.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uT7laJJodj8",
        "outputId": "cb1cb760-5d77-46e1-9fbf-a1d8506220f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 1036170 bytes (1012 KiB)\n",
            "\n",
            "Extracting archive: enron_814.zip\n",
            "--\n",
            "Path = enron_814.zip\n",
            "Type = zip\n",
            "Physical Size = 1036170\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b\n",
            "Would you like to replace the existing file:\n",
            "  Path:     ./__MACOSX/._enron_814\n",
            "  Size:     176 bytes (1 KiB)\n",
            "  Modified: 2025-01-17 20:21:10\n",
            "with the file from archive:\n",
            "  Path:     __MACOSX/._enron_814\n",
            "  Size:     176 bytes (1 KiB)\n",
            "  Modified: 2025-01-17 20:21:10\n",
            "? (Y)es / (N)o / (A)lways / (S)kip all / A(u)to rename all / (Q)uit? y\n",
            "\n",
            "  0% 1 - __MACOSX/._enron_814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Would you like to replace the existing file:\n",
            "  Path:     ./enron_814/36.\n",
            "  Size:     2948 bytes (3 KiB)\n",
            "  Modified: 2004-02-04 01:19:54\n",
            "with the file from archive:\n",
            "  Path:     enron_814/36.\n",
            "  Size:     2948 bytes (3 KiB)\n",
            "  Modified: 2004-02-04 01:19:54\n",
            "? (Y)es / (N)o / (A)lways / (S)kip all / A(u)to rename all / (Q)uit? a\n",
            "\n",
            "  0% 2 - enron_814/36.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 460 - enron_814/89.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 880 - enron_814/355.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 1334 - enron_814/85.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Folders: 1\n",
            "Files: 1629\n",
            "Size:       1334204\n",
            "Compressed: 1036170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IfHGvybCysQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d590bb94-597e-4952-fb31-4aa3a30cfba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"Document-ID\": \"19325260.1075857596087\",\n",
            "        \"content\": \"i heard he's interviewing to be Shankman's right hand manJennifer Burns10/20/2000 02:12 PMTo: John Arnold/HOU/ECT@ECTcc:  Subject: I saw Bill Perkins today, he was on 33.  I heard someone call my name and I was like hey Bill.  Weird huh?\"\n",
            "    },\n",
            "    {\n",
            "        \"Document-ID\": \"33025919.1075857594206\",\n",
            "        \"content\": \"saw a lot of the bulls sell summer against length in front to mitigate margins/absolute position limits/var.  as these guys are taking off the front, they are also buying back summer.  el paso large buyer of next winter today taking off spreads.  certainly a reason why the spreads were so strong on the way up and such a piece now.   really the only one left with any risk premium built in is h/j now.   it was trading equivalent of 180 on access, down 40+ from this morning.  certainly if we are entering a period of bearish to neutral trade, h/j will get whacked.  certainly understand the arguments for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was trading for 55 on monday.  today it was 10/17.  the market's view of probability of h going crazy has certainly changed in past 48 hours and that has to be reflected in h/j.slafontaine@globalp.com on 12/13/2000 04:15:51 PMTo: slafontaine@globalp.comcc: John.Arnold@enron.com Subject: re:spreadsmkt getting a little more bearish the back of winter i think-if we get anothercold blast jan/feb mite move out. with oil moving down and march closer flat pxwide to jan im not so bearish these sprds now-less bullish march april as well.\"\n",
            "    }\n",
            "]\n",
            "814\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# please print out the first 2 docs\n",
        "import re, os, json\n",
        "Mails=[]\n",
        "for file in os.listdir(\"enron_814\"):\n",
        "  with open(os.path.join(\"enron_814\",file), 'r', encoding='utf-8') as f:\n",
        "    mail= f.read()\n",
        "    doc={}\n",
        "    match = re.search(r\"(\\d+\\.\\d+)\", mail)\n",
        "    if match:\n",
        "      document_id = match.group(1)\n",
        "      # print(document_id)\n",
        "      doc[\"Document-ID\"]=document_id\n",
        "      # doc[\"content\"]=contents\n",
        "      content=mail.split(\"\\n\")[16:]\n",
        "      content_str=\"\"\n",
        "      for i in content:\n",
        "        content_str+=i\n",
        "      doc['content']=content_str\n",
        "      Mails.append(doc)\n",
        "\n",
        "# print(Mails)\n",
        "print(json.dumps(Mails[:2], indent=4, ensure_ascii=False))\n",
        "print(len(Mails))\n",
        "# print(content)\n",
        "# print(mail)\n",
        "\n",
        "# print(contents.split(\"\\n\")[16:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-juuk1zLuw"
      },
      "source": [
        "Now that you can read the documents, let's move on to tokenization. We are going to simplify things for you:\n",
        "1. You should **lowercase** all words.\n",
        "2. Replace line breaks (e.g., \\n, \\n\\n), punctuations, dashes and splash (e.g., -, /) and special characters (\\u2019, \\u2005, etc.) with empty space (\" \").\n",
        "3. Tokenize the documents by splitting on whitespace.\n",
        "4. Then only keep words that have a-zA-Z in them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_CpG2ifejjvx"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
        "    return tokens\n",
        "tokens=[]\n",
        "for mail in Mails:\n",
        "  content=mail['content']\n",
        "  tks=custom_tokenize(content)\n",
        "  for tk in tks:\n",
        "    tokens.append(tk)\n",
        "# print(len(set(tokens)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tst9U9DxN-Jp"
      },
      "source": [
        "## Print the first two documents after tokenizing (5 points)\n",
        "\n",
        "Once you have your parser working, you should print the first two documents (documentID and tokens).\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "* DocumentID Tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WcJbBEQgJUd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "dcc17b51-eb39-496b-81da-88bdf3f87674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"Document-ID\": \"19325260.1075857596087\",\n",
            "        \"content\": \"i heard he's interviewing to be Shankman's right hand manJennifer Burns10/20/2000 02:12 PMTo: John Arnold/HOU/ECT@ECTcc:  Subject: I saw Bill Perkins today, he was on 33.  I heard someone call my name and I was like hey Bill.  Weird huh?\",\n",
            "        \"tokens\": [\n",
            "            \"i\",\n",
            "            \"heard\",\n",
            "            \"he\",\n",
            "            \"s\",\n",
            "            \"interviewing\",\n",
            "            \"to\",\n",
            "            \"be\",\n",
            "            \"shankman\",\n",
            "            \"s\",\n",
            "            \"right\",\n",
            "            \"hand\",\n",
            "            \"manjennifer\",\n",
            "            \"burns10\",\n",
            "            \"pmto\",\n",
            "            \"john\",\n",
            "            \"arnold\",\n",
            "            \"hou\",\n",
            "            \"ect\",\n",
            "            \"ectcc\",\n",
            "            \"subject\",\n",
            "            \"i\",\n",
            "            \"saw\",\n",
            "            \"bill\",\n",
            "            \"perkins\",\n",
            "            \"today\",\n",
            "            \"he\",\n",
            "            \"was\",\n",
            "            \"on\",\n",
            "            \"i\",\n",
            "            \"heard\",\n",
            "            \"someone\",\n",
            "            \"call\",\n",
            "            \"my\",\n",
            "            \"name\",\n",
            "            \"and\",\n",
            "            \"i\",\n",
            "            \"was\",\n",
            "            \"like\",\n",
            "            \"hey\",\n",
            "            \"bill\",\n",
            "            \"weird\",\n",
            "            \"huh\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"Document-ID\": \"33025919.1075857594206\",\n",
            "        \"content\": \"saw a lot of the bulls sell summer against length in front to mitigate margins/absolute position limits/var.  as these guys are taking off the front, they are also buying back summer.  el paso large buyer of next winter today taking off spreads.  certainly a reason why the spreads were so strong on the way up and such a piece now.   really the only one left with any risk premium built in is h/j now.   it was trading equivalent of 180 on access, down 40+ from this morning.  certainly if we are entering a period of bearish to neutral trade, h/j will get whacked.  certainly understand the arguments for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was trading for 55 on monday.  today it was 10/17.  the market's view of probability of h going crazy has certainly changed in past 48 hours and that has to be reflected in h/j.slafontaine@globalp.com on 12/13/2000 04:15:51 PMTo: slafontaine@globalp.comcc: John.Arnold@enron.com Subject: re:spreadsmkt getting a little more bearish the back of winter i think-if we get anothercold blast jan/feb mite move out. with oil moving down and march closer flat pxwide to jan im not so bearish these sprds now-less bullish march april as well.\",\n",
            "        \"tokens\": [\n",
            "            \"saw\",\n",
            "            \"a\",\n",
            "            \"lot\",\n",
            "            \"of\",\n",
            "            \"the\",\n",
            "            \"bulls\",\n",
            "            \"sell\",\n",
            "            \"summer\",\n",
            "            \"against\",\n",
            "            \"length\",\n",
            "            \"in\",\n",
            "            \"front\",\n",
            "            \"to\",\n",
            "            \"mitigate\",\n",
            "            \"margins\",\n",
            "            \"absolute\",\n",
            "            \"position\",\n",
            "            \"limits\",\n",
            "            \"var\",\n",
            "            \"as\",\n",
            "            \"these\",\n",
            "            \"guys\",\n",
            "            \"are\",\n",
            "            \"taking\",\n",
            "            \"off\",\n",
            "            \"the\",\n",
            "            \"front\",\n",
            "            \"they\",\n",
            "            \"are\",\n",
            "            \"also\",\n",
            "            \"buying\",\n",
            "            \"back\",\n",
            "            \"summer\",\n",
            "            \"el\",\n",
            "            \"paso\",\n",
            "            \"large\",\n",
            "            \"buyer\",\n",
            "            \"of\",\n",
            "            \"next\",\n",
            "            \"winter\",\n",
            "            \"today\",\n",
            "            \"taking\",\n",
            "            \"off\",\n",
            "            \"spreads\",\n",
            "            \"certainly\",\n",
            "            \"a\",\n",
            "            \"reason\",\n",
            "            \"why\",\n",
            "            \"the\",\n",
            "            \"spreads\",\n",
            "            \"were\",\n",
            "            \"so\",\n",
            "            \"strong\",\n",
            "            \"on\",\n",
            "            \"the\",\n",
            "            \"way\",\n",
            "            \"up\",\n",
            "            \"and\",\n",
            "            \"such\",\n",
            "            \"a\",\n",
            "            \"piece\",\n",
            "            \"now\",\n",
            "            \"really\",\n",
            "            \"the\",\n",
            "            \"only\",\n",
            "            \"one\",\n",
            "            \"left\",\n",
            "            \"with\",\n",
            "            \"any\",\n",
            "            \"risk\",\n",
            "            \"premium\",\n",
            "            \"built\",\n",
            "            \"in\",\n",
            "            \"is\",\n",
            "            \"h\",\n",
            "            \"j\",\n",
            "            \"now\",\n",
            "            \"it\",\n",
            "            \"was\",\n",
            "            \"trading\",\n",
            "            \"equivalent\",\n",
            "            \"of\",\n",
            "            \"on\",\n",
            "            \"access\",\n",
            "            \"down\",\n",
            "            \"from\",\n",
            "            \"this\",\n",
            "            \"morning\",\n",
            "            \"certainly\",\n",
            "            \"if\",\n",
            "            \"we\",\n",
            "            \"are\",\n",
            "            \"entering\",\n",
            "            \"a\",\n",
            "            \"period\",\n",
            "            \"of\",\n",
            "            \"bearish\",\n",
            "            \"to\",\n",
            "            \"neutral\",\n",
            "            \"trade\",\n",
            "            \"h\",\n",
            "            \"j\",\n",
            "            \"will\",\n",
            "            \"get\",\n",
            "            \"whacked\",\n",
            "            \"certainly\",\n",
            "            \"understand\",\n",
            "            \"the\",\n",
            "            \"arguments\",\n",
            "            \"for\",\n",
            "            \"h\",\n",
            "            \"j\",\n",
            "            \"if\",\n",
            "            \"h\",\n",
            "            \"settles\",\n",
            "            \"that\",\n",
            "            \"spread\",\n",
            "            \"is\",\n",
            "            \"probably\",\n",
            "            \"worth\",\n",
            "            \"h\",\n",
            "            \"call\",\n",
            "            \"was\",\n",
            "            \"trading\",\n",
            "            \"for\",\n",
            "            \"on\",\n",
            "            \"monday\",\n",
            "            \"today\",\n",
            "            \"it\",\n",
            "            \"was\",\n",
            "            \"the\",\n",
            "            \"market\",\n",
            "            \"s\",\n",
            "            \"view\",\n",
            "            \"of\",\n",
            "            \"probability\",\n",
            "            \"of\",\n",
            "            \"h\",\n",
            "            \"going\",\n",
            "            \"crazy\",\n",
            "            \"has\",\n",
            "            \"certainly\",\n",
            "            \"changed\",\n",
            "            \"in\",\n",
            "            \"past\",\n",
            "            \"hours\",\n",
            "            \"and\",\n",
            "            \"that\",\n",
            "            \"has\",\n",
            "            \"to\",\n",
            "            \"be\",\n",
            "            \"reflected\",\n",
            "            \"in\",\n",
            "            \"h\",\n",
            "            \"j\",\n",
            "            \"slafontaine\",\n",
            "            \"globalp\",\n",
            "            \"com\",\n",
            "            \"on\",\n",
            "            \"pmto\",\n",
            "            \"slafontaine\",\n",
            "            \"globalp\",\n",
            "            \"comcc\",\n",
            "            \"john\",\n",
            "            \"arnold\",\n",
            "            \"enron\",\n",
            "            \"com\",\n",
            "            \"subject\",\n",
            "            \"re\",\n",
            "            \"spreadsmkt\",\n",
            "            \"getting\",\n",
            "            \"a\",\n",
            "            \"little\",\n",
            "            \"more\",\n",
            "            \"bearish\",\n",
            "            \"the\",\n",
            "            \"back\",\n",
            "            \"of\",\n",
            "            \"winter\",\n",
            "            \"i\",\n",
            "            \"think\",\n",
            "            \"if\",\n",
            "            \"we\",\n",
            "            \"get\",\n",
            "            \"anothercold\",\n",
            "            \"blast\",\n",
            "            \"jan\",\n",
            "            \"feb\",\n",
            "            \"mite\",\n",
            "            \"move\",\n",
            "            \"out\",\n",
            "            \"with\",\n",
            "            \"oil\",\n",
            "            \"moving\",\n",
            "            \"down\",\n",
            "            \"and\",\n",
            "            \"march\",\n",
            "            \"closer\",\n",
            "            \"flat\",\n",
            "            \"pxwide\",\n",
            "            \"to\",\n",
            "            \"jan\",\n",
            "            \"im\",\n",
            "            \"not\",\n",
            "            \"so\",\n",
            "            \"bearish\",\n",
            "            \"these\",\n",
            "            \"sprds\",\n",
            "            \"now\",\n",
            "            \"less\",\n",
            "            \"bullish\",\n",
            "            \"march\",\n",
            "            \"april\",\n",
            "            \"as\",\n",
            "            \"well\"\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# your code and output here\n",
        "for mail in Mails:\n",
        "  mail['tokens']=custom_tokenize(mail['content'])\n",
        "print(json.dumps(Mails[:2], indent=4, ensure_ascii=False))\n",
        "# print(Mails[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2yVi76Ij1ys"
      },
      "source": [
        "## Dictionary Size (5 points)\n",
        "\n",
        "Next you should report the size of your dictionary, that is, how many unique tokens among all the documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "53bAA66zkI55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07948c3b-d622-494f-ee98-e9ff3dad370f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of Unique Tokens: 12380\n"
          ]
        }
      ],
      "source": [
        "print(\"No.of Unique Tokens:\",len(set(tokens)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU928IKKkJGY"
      },
      "source": [
        "## Top-20 Words (5 points)\n",
        "\n",
        "Finally, you should print a list of the top-20 most popular words by counting among all documents.\n",
        "\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "* Rank. Token, Count\n",
        "\n",
        "1. awesome, 20\n",
        "2. cool, 15\n",
        "3. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8MS6n6oNklvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1b490056-8a62-433b-ae14-fd017e4af0e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 3822),\n",
              " ('to', 2886),\n",
              " ('ect', 2190),\n",
              " ('enron', 2053),\n",
              " ('a', 1807),\n",
              " ('and', 1704),\n",
              " ('you', 1683),\n",
              " ('of', 1632),\n",
              " ('i', 1609),\n",
              " ('on', 1532),\n",
              " ('in', 1400),\n",
              " ('hou', 1345),\n",
              " ('john', 1333),\n",
              " ('is', 1151),\n",
              " ('for', 1110),\n",
              " ('com', 1087),\n",
              " ('arnold', 943),\n",
              " ('subject', 846),\n",
              " ('it', 844),\n",
              " ('s', 828)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter(tokens)\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_word_counts[:20]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcrFD2F-N-Jp"
      },
      "source": [
        "# Part 2: Boolean Retrieval (30 points)\n",
        "\n",
        "In this part you will build an inverted index to support Boolean retrieval. You should use the tokenization strategy from above.\n",
        "\n",
        "We require your index to support AND, OR, NOT queries.\n",
        "\n",
        "Search for the queries below using your index and print out matching documents (for each query, print out 5 matching documents):\n",
        "* buyer\n",
        "* margins AND limits\n",
        "* winter OR summer\n",
        "* buyers AND risk AND NOT crazy\n",
        "* never OR know\n",
        "\n",
        "Recall, that you should apply the exact same pre-processing strategies to the query as we do to the documents.\n",
        "\n",
        "The output should like this:\n",
        "* DocumentID Document\n",
        "\n",
        "To make our life easier, please output the DocumentIDs in lexicographic order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "GS3Tc_kYN-Jp"
      },
      "outputs": [],
      "source": [
        "# build the index here\n",
        "# add cells as needed to organize your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82YUVPUknOjX"
      },
      "source": [
        "## Running the five queries (4 points each, 20 points in total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "qWH8h5ZkN-Jp"
      },
      "outputs": [],
      "source": [
        "# search for the input using your index and print out ids of matching documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzx6tpmpjBsX"
      },
      "source": [
        "Now show the results for the query: `buyer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gUOUjF18jFTT"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbJ6yn3yMQWY"
      },
      "source": [
        "*Now* show the results for the query: `margins AND limits`\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SmF62rQ_MRlO"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KlaH0fMTO5"
      },
      "source": [
        "Now show the results for the query: `winter OR summer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MAFBtPGmMVRW"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVcjjsBWIFv"
      },
      "source": [
        "Now show the results for the query: `buyers AND risk AND NOT crazy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3e7Ipk2JWIF5"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ0sd8zrWIbb"
      },
      "source": [
        "Now show the results for the query: `never OR know`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g2BumYswWIbc"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAhHuIDfN-Jp"
      },
      "source": [
        "## Observations (10 points)\n",
        "Does your boolean search engine find relevant documents for these queries? As in, would our customers be happy if we shipped this retrieval engine? Why or why not?\n",
        "\n",
        "What is the impact of the pre-processing options? Do they impact your search quality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX6k-Vil0GIy"
      },
      "source": [
        "*your discussion here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5JYoNNrlimy"
      },
      "source": [
        "# Part 3: Ranking (35 points)\n",
        "\n",
        "For the third part, you will add ranking to your search system. Given a search query, our goal is to retrieve the top-5 most relevant emails by assigning a score to each document.\n",
        "\n",
        "We will explore two ranking methods, each offering a different approach to scoring and ranking documents:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMlmJ-8mP8X"
      },
      "source": [
        "### Ranking method A: Ranking with vector space model with TF-IDF (15 points)\n",
        "\n",
        "**Cosine:** You should use cosine as your scoring function.\n",
        "\n",
        "**TFIDF:** For the **document vectors**, use the standard TF-IDF scores introduced in class. For the **query vector**, use **simple weights (the raw term frequency)**. For example:\n",
        "* query: never $\\rightarrow$ (1)\n",
        "* query: never know $\\rightarrow$ (1, 1)\n",
        "\n",
        "**Query:**  `trade`\n",
        "\n",
        "**Output:**\n",
        "You should output the top-5 results plus the cosine score of each of these documents.  \n",
        "\n",
        "The output should be like this:\n",
        "\n",
        "Rank Scores DocumentID Document\n",
        "\n",
        "---\n",
        "\n",
        "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3OMOP3bomAM2"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "\n",
        "# print out the top-5 retrieved emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf3-HsRamBkh"
      },
      "source": [
        "### Ranking method B: Ranking with BM25 (15 points)\n",
        "Finally, let's try the BM25 approach for ranking. Refer to https://en.wikipedia.org/wiki/Okapi_BM25 for the specific formula. You could choose k_1 = 1.2 and b = 0.75 but feel free to try other options.\n",
        "\n",
        "**Query:**  `gas floor`\n",
        "\n",
        "**Output:**\n",
        "You should output the top-5 results plus the BM25 score of each of these documents.  \n",
        "\n",
        "The output should be like this:\n",
        "\n",
        "Rank Scores DocumentID Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lESwxZNImIle"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "\n",
        "# print out the top-5 retrieved emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dScZI3PvnNqV"
      },
      "source": [
        "## Observations (5 points)\n",
        "What do you observe? Are there key differences between the two ranking approaches? Briefly discuss in bullet points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gku0X4oFrBRX"
      },
      "source": [
        "* Your observations:\n",
        "* Differences:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwQKelDxgzS8"
      },
      "source": [
        "# Part 4: Cool LLM RAG Extension (15 points)\n",
        "\n",
        "Finally, we give you an opportunity to explore using OASIS for Retrieval-Augmented Generation (RAG) with LLMs.\n",
        "Here, the task is to retrieve the top-5 emails for a query you like. You will then pass the retrieved email content along with your question to the LLM and let it answer your question. Specifically, we want you:\n",
        "\n",
        "* Query the LLM directly with your question.\n",
        "* Retrieve the top-5 emails based on your query and pass them to the LLM along with your question.\n",
        "* How is the RAG output different from the non-RAG output? Does retrieval help the LLM better answer your question?\n",
        "\n",
        "We recommend using Gemini following the [instructions](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb) here.\n",
        "\n",
        "Hint: Take a close look at the dataset and pick a specific, relevant query where retrieval can enhance the LLM’s response.\n",
        "\n",
        "*What You Will Submit*\n",
        "- Your query.\n",
        "- The top-5 retrieved emails (including Document ID and ranking score).\n",
        "- The LLM's response without retrieval.\n",
        "- The LLM's response with retrieval (RAG).\n",
        "- A brief analysis comparing both outputs.\n",
        "\n",
        "We will grade this last part according to correctness, effort, and creativity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irxiT-iTgzS9"
      },
      "source": [
        "## Step 1: Query the LLM Directly (Without Retrieval) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JiyOf67igzS9"
      },
      "outputs": [],
      "source": [
        "# your code here to prompt LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otHscQFhgzTN"
      },
      "source": [
        "## Step 2: Query the LLM with Retrieved Emails (RAG) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TnArY1PbgzTN"
      },
      "outputs": [],
      "source": [
        "# print out the LLM response without the email content\n",
        "\n",
        "\n",
        "# print out the LLM response with the email content (RAG)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YnqxBZ4gzTN"
      },
      "source": [
        "## Discussion (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyVyTcM-gzTN"
      },
      "source": [
        "In this section, reflect on the performance of different ranking methods and the impact of retrieval on LLM responses. Consider:\n",
        "\n",
        "- How did retrieval affect the LLM’s response? Did it improve factual accuracy or relevance?\n",
        "- Were there cases where retrieval hurt performance (e.g., irrelevant documents, redundancy)?\n",
        "- Any ideas for improving the ranking or retrieval process?\n",
        "\n",
        "Keep your discussion *concise* and *insightful*, focusing on key takeaways from your experiments. Please answer in bullet points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMjckcmgzTN"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "* point 1\n",
        "* point 2\n",
        "* ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FihN3qniN-Jy"
      },
      "source": [
        "# Collaboration Declarations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKp_Fe-N-Jy"
      },
      "source": [
        "*You should fill out your collaboration declarations here.*\n",
        "\n",
        "https://stackoverflow.com/questions/12897374/get-unique-values-from-a-list-in-python\n",
        "\n",
        "ChatGPT:\n",
        "- i want to no. of occurences of each element in a list"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ASjn0YYqcAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}