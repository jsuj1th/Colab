{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsuj1th/Colab/blob/main/33500274_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAxczUGVN-Jc"
      },
      "source": [
        "#### CSCE 670 :: Information Storage & Retrieval :: Texas A&M University :: Spring 2025\n",
        "\n",
        "\n",
        "# Homework 1:  OASIS Search Engine, The Beginning\n",
        "\n",
        "### 100 points [4% of your final grade]\n",
        "\n",
        "### Due: February 5 (Wednesday) by 11:59pm\n",
        "\n",
        "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine + explore some LLM capabilities.\n",
        "\n",
        "*Submission instructions (Canvas):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via Canvas (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
        "\n",
        "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
        "\n",
        "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Canvas, search StackOverflow, even use ChatGPT. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. See the course syllabus for details.\n",
        "\n",
        "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
        "\n",
        "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CKllFl-N-Jk"
      },
      "source": [
        "# Dataset: Enron Email Dataset\n",
        "\n",
        "We are providing you with a small collection of emails from the Enron Email Dataset. The Enron Email Dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron. The full corpus contains a total of about 0.5M messages (https://www.cs.cmu.edu/~enron/). For this homework, we will use a small subset of the data. The subset contains 814 emails extracted from the `_sent_mail` of Arnold-J. We have zipped the 814 files (each file contains the information of an email). The zipped file is available on Canvas as `enron_814.zip`. The subset we provide is about 1.1MB. You should treat each email as a unique document to be indexed by your system. You can download the data from Canvas to your local filesystem. We're going to use these emails as the basis of OASIS, our Open Access Searchable Information System!\n",
        "\n",
        "\n",
        "Below is an example of one email.\n",
        "\n",
        "```text\n",
        "Message-ID: <33025919.1075857594206.JavaMail.evans@thyme>\n",
        "Date: Wed, 13 Dec 2000 13:09:00 -0800 (PST)\n",
        "From: john.arnold@enron.com\n",
        "To: slafontaine@globalp.com\n",
        "Subject: re:spreads\n",
        "Mime-Version: 1.0\n",
        "Content-Type: text/plain; charset=us-ascii\n",
        "Content-Transfer-Encoding: 7bit\n",
        "X-From: John Arnold\n",
        "X-To: slafontaine@globalp.com @ ENRON\n",
        "X-cc:\n",
        "X-bcc:\n",
        "X-Folder: \\John_Arnold_Dec2000\\Notes Folders\\'sent mail\n",
        "X-Origin: Arnold-J\n",
        "X-FileName: Jarnold.nsf\n",
        "\n",
        "saw a lot of the bulls sell summer against length in front to mitigate\n",
        "margins/absolute position limits/var.  as these guys are taking off the\n",
        "front, they are also buying back summer.  el paso large buyer of next winter\n",
        "today taking off spreads.  certainly a reason why the spreads were so strong\n",
        "on the way up and such a piece now.   really the only one left with any risk\n",
        "premium built in is h/j now.   it was trading equivalent of 180 on access,\n",
        "down 40+ from this morning.  certainly if we are entering a period of bearish\n",
        "to neutral trade, h/j will get whacked.  certainly understand the arguments\n",
        "for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was\n",
        "trading for 55 on monday.  today it was 10/17.  the market's view of\n",
        "probability of h going crazy has certainly changed in past 48 hours and that\n",
        "has to be reflected in h/j.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "slafontaine@globalp.com on 12/13/2000 04:15:51 PM\n",
        "To: slafontaine@globalp.com\n",
        "cc: John.Arnold@enron.com\n",
        "Subject: re:spreads\n",
        "\n",
        "\n",
        "\n",
        "mkt getting a little more bearish the back of winter i think-if we get another\n",
        "cold blast jan/feb mite move out. with oil moving down and march closer flat\n",
        "px\n",
        "wide to jan im not so bearish these sprds now-less bullish march april as\n",
        "well.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaiOjFANN-Jo"
      },
      "source": [
        "# Part 1: Read and Parse the Email Data (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoiNbqQDrVQ2"
      },
      "source": [
        "Recall how we handled file input in Homework 0? Well, here, our goal is to read the emails so that we can begin to tokenize them later. For this step, you should read the dataset and print the emails. Note that our dataset contains multiple files. You will need to write Python code to read from these files, and then build a list to store the documents. Each item in the list should be a dictionary containing the `Document-ID` as the key, and email content as the value. You can discard all the supplementary information of the email, e.g., `Date`, `From`, `To`, `Subject`, etc.\n",
        "\n",
        "A document should look like:\n",
        "\n",
        "```text\n",
        "{'Document-ID': '33025919.1075857594206',\n",
        "'content': 'saw a lot of the bulls sell summer against length in front to mitigate\n",
        "margins/absolute position limits/var.  as these guys are taking off the\n",
        "front, they are also buying back summer.  el paso large buyer of next winter\n",
        "today taking off spreads.  certainly a reason why the spreads were so strong\n",
        "on the way up and such a piece now.   really the only one left with any risk\n",
        "premium built in is h/j now.   it was trading equivalent of 180 on access,\n",
        "down 40+ from this morning.  certainly if we are entering a period of bearish\n",
        "to neutral trade, h/j will get whacked.  certainly understand the arguments\n",
        "for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was\n",
        "trading for 55 on monday.  today it was 10/17.  the market's view of\n",
        "probability of h going crazy has certainly changed in past 48 hours and that\n",
        "has to be reflected in h/j.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "slafontaine@globalp.com on 12/13/2000 04:15:51 PM\n",
        "To: slafontaine@globalp.com\n",
        "cc: John.Arnold@enron.com\n",
        "Subject: re:spreads\n",
        "\n",
        "\n",
        "\n",
        "mkt getting a little more bearish the back of winter i think-if we get another\n",
        "cold blast jan/feb mite move out. with oil moving down and march closer flat\n",
        "px\n",
        "wide to jan im not so bearish these sprds now-less bullish march april as\n",
        "well.'\n",
        "}\n",
        "```\n",
        "\n",
        "For this homework, you should treat the email content as a document and the Message-ID as the document ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0sMQsv-mg1l"
      },
      "source": [
        "## Print the first two documents (5 points)\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "DocumentID Document\n",
        "\n",
        "33025919.1075857594206 saw a lot of the bulls sell summer ......\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uT7laJJodj8",
        "outputId": "e208a57a-6ee4-4b63-aa62-fd98e4bbd36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 1036170 bytes (1012 KiB)\n",
            "\n",
            "Extracting archive: enron_814.zip\n",
            "--\n",
            "Path = enron_814.zip\n",
            "Type = zip\n",
            "Physical Size = 1036170\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b\n",
            "Would you like to replace the existing file:\n",
            "  Path:     ./__MACOSX/._enron_814\n",
            "  Size:     176 bytes (1 KiB)\n",
            "  Modified: 2025-01-17 20:21:10\n",
            "with the file from archive:\n",
            "  Path:     __MACOSX/._enron_814\n",
            "  Size:     176 bytes (1 KiB)\n",
            "  Modified: 2025-01-17 20:21:10\n",
            "? (Y)es / (N)o / (A)lways / (S)kip all / A(u)to rename all / (Q)uit? A\n",
            "\n",
            "  0% 1 - __MACOSX/._enron_814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 317 - __MACOSX/enron_814/._158.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 626 - enron_814/205.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 944 - enron_814/5.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 1232 - enron_814/344.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 1535 - __MACOSX/enron_814/._758.\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Folders: 1\n",
            "Files: 1629\n",
            "Size:       1334204\n",
            "Compressed: 1036170\n"
          ]
        }
      ],
      "source": [
        "!apt-get install p7zip-full\n",
        "!7z x enron_814.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IfHGvybCysQK",
        "outputId": "cdd52ebe-5e5d-4ba6-b14a-d54e303c6ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10023450.1075857650138 ---------------------- Forwarded by John Arnold/HOU/ECT on 05/10/2001 02:21 \n",
            "PM ---------------------------\n",
            "\n",
            "\n",
            "\"Schaefer, Matthew\" <MSchaefer@NYMEX.com> on 05/10/2001 01:26:57 PM\n",
            "To: Brad Banky <bbanky@txu.com>, David Rosenberg \n",
            "<david.m.rosenberg@enron.com>, James Haupt <jay.haupt@eprime.com>, Jeff Frase \n",
            "<jeff.frase@gs.com>, Jeff Ong <glyons@tractabelusa.com>, Jim Adams \n",
            "<james.l.adams@usa.conoco.com>, John Arnold <jarnold@enron.com>, Kayvan Scott \n",
            "Malek <ksmalek@aep.com>, Michael Maggi <mike.maggi@enron.com>, Robert Collins \n",
            "<collinsb2@epenergy.com>, Russ Knutsen <rrkn@chevron.com>, Sanjiv Khosla \n",
            "<sanjiv.khosla@msdw.com>, William Coorsh <bcoorsh@tractabelusa.com>\n",
            "cc:  \n",
            "Subject: Option Advisory Committee Meeting May 31\n",
            "\n",
            "\n",
            "Please be advised that the Option Advisory Committee meeting will be at 4:00\n",
            "eastern time, May 31, 2001.\n",
            "\n",
            "\n",
            "Matthew Schaefer\n",
            "New York Mercantile Exchange\n",
            "212.299.2612\n",
            "\n",
            "\n",
            "\n",
            "17350292.1075857654244 71\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "george.ellis@americas.bnpparibas.com on 03/14/2001 08:18:25 AM\n",
            "To: george.ellis@americas.bnpparibas.com\n",
            "cc:  \n",
            "Subject: BNP PARIBAS Commodity Futures Weekly AGA Survey\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Good Morning,\n",
            "\n",
            "Just a reminder to get your AGA estimates in by Noon EST (11:00 CST) TODAY.\n",
            "\n",
            "Last Year      -31\n",
            "Last Week      -73\n",
            "\n",
            "Thank You,\n",
            "George Ellis\n",
            "BNP PARIBAS Commodity Futures, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "______________________________________________________________________________\n",
            "_______________________________________________________\n",
            "\n",
            "Ce message et toutes les pieces jointes (ci-apres le \"message\") sont etablis \n",
            "a l'intention exclusive de ses destinataires et sont confidentiels. Si vous \n",
            "recevez ce message par erreur, merci de le detruire et d'en avertir \n",
            "immediatement l'expediteur.\n",
            "\n",
            "Toute utilisation de ce message non conforme a sa destination, toute \n",
            "diffusion ou toute publication, totale ou partielle, est interdite, sauf \n",
            "autorisation expresse.\n",
            "\n",
            "L'internet ne permettant pas d'assurer l'integrite de ce message, BNP PARIBAS \n",
            "(et ses filiales) decline(nt) toute responsabilite au titre de ce message, \n",
            "dans l'hypothese ou il aurait ete modifie.\n",
            " \n",
            "------------------------------------------------------------------------------\n",
            "----\n",
            "This message and any attachments (the \"message\") are intended solely for the \n",
            "addressees and are confidential. If you receive this message in error, please \n",
            "delete it and immediately notify the sender.\n",
            "\n",
            "Any use not in accord with its purpose, any dissemination or disclosure, \n",
            "either whole or partial, is prohibited except formal approval.\n",
            "\n",
            "The internet can not guarantee the integrity of this message. BNP PARIBAS \n",
            "(and its subsidiaries) shall (will) not therefore be liable for the message \n",
            "if modified.\n",
            "______________________________________________________________________________\n",
            "_______________________________________________________\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# please print out the first 2 docs\n",
        "import re, os, json\n",
        "from itertools import islice\n",
        "Mails=[]\n",
        "doc={}\n",
        "i=1\n",
        "for file in os.listdir(\"enron_814\"):\n",
        "  # print(file)\n",
        "\n",
        "  with open(os.path.join(\"enron_814\",file), 'r', encoding='utf-8') as f:\n",
        "    mail= f.read()\n",
        "\n",
        "    match = re.search(r\"(\\d+\\.\\d+)\", mail)\n",
        "    content_match=re.search(r\"(\\r?\\n){2,}([\\s\\S]+)\", mail)\n",
        "    if match:\n",
        "      document_id = match.group(1)\n",
        "      content=content_match.group(2)\n",
        "      doc[document_id]=content\n",
        "\n",
        "for key, value in islice(doc.items(), 2):\n",
        "    print(key, value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-juuk1zLuw"
      },
      "source": [
        "Now that you can read the documents, let's move on to tokenization. We are going to simplify things for you:\n",
        "1. You should **lowercase** all words.\n",
        "2. Replace line breaks (e.g., \\n, \\n\\n), punctuations, dashes and splash (e.g., -, /) and special characters (\\u2019, \\u2005, etc.) with empty space (\" \").\n",
        "3. Tokenize the documents by splitting on whitespace.\n",
        "4. Then only keep words that have a-zA-Z in them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CpG2ifejjvx",
        "outputId": "410be7d2-66e2-48c7-ed20-812bd8896554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9410\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'[\\n\\r\\t.:]', ' ', text)\n",
        "    text=re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
        "    return tokens\n",
        "\n",
        "tokens=[]\n",
        "for id,mail in doc.items():\n",
        "  tks=custom_tokenize(mail)\n",
        "  for tk in tks:\n",
        "    tokens.append(tk)\n",
        "print(len(set(tokens)))\n",
        "lookup_doc=doc.copy()\n",
        "# print(doc)\n",
        "# print(lookup_doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tst9U9DxN-Jp"
      },
      "source": [
        "## Print the first two documents after tokenizing (5 points)\n",
        "\n",
        "Once you have your parser working, you should print the first two documents (documentID and tokens).\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "* DocumentID Tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcJbBEQgJUd8"
      },
      "outputs": [],
      "source": [
        "# your code and output here\n",
        "for id,mail in doc.items():\n",
        "  doc[id]=custom_tokenize(mail)\n",
        "# print(lookup_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXqQ6uL2ZysM",
        "outputId": "d4d7b232-4293-420c-85f6-6e610c183c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10023450.1075857650138 ['forwarded', 'by', 'john', 'arnold', 'hou', 'ect', 'on', 'pm', 'schaefer', 'matthew', 'mschaefer', 'nymex', 'com', 'on', 'pm', 'to', 'brad', 'banky', 'bbanky', 'txu', 'com', 'david', 'rosenberg', 'david', 'm', 'rosenberg', 'enron', 'com', 'james', 'haupt', 'jay', 'haupt', 'eprime', 'com', 'jeff', 'frase', 'jeff', 'frase', 'gs', 'com', 'jeff', 'ong', 'glyons', 'tractabelusa', 'com', 'jim', 'adams', 'james', 'l', 'adams', 'usa', 'conoco', 'com', 'john', 'arnold', 'jarnold', 'enron', 'com', 'kayvan', 'scott', 'malek', 'ksmalek', 'aep', 'com', 'michael', 'maggi', 'mike', 'maggi', 'enron', 'com', 'robert', 'collins', 'collinsb', 'epenergy', 'com', 'russ', 'knutsen', 'rrkn', 'chevron', 'com', 'sanjiv', 'khosla', 'sanjiv', 'khosla', 'msdw', 'com', 'william', 'coorsh', 'bcoorsh', 'tractabelusa', 'com', 'cc', 'subject', 'option', 'advisory', 'committee', 'meeting', 'may', 'please', 'be', 'advised', 'that', 'the', 'option', 'advisory', 'committee', 'meeting', 'will', 'be', 'at', 'eastern', 'time', 'may', 'matthew', 'schaefer', 'new', 'york', 'mercantile', 'exchange']\n",
            "17350292.1075857654244 ['george', 'ellis', 'americas', 'bnpparibas', 'com', 'on', 'am', 'to', 'george', 'ellis', 'americas', 'bnpparibas', 'com', 'cc', 'subject', 'bnp', 'paribas', 'commodity', 'futures', 'weekly', 'aga', 'survey', 'good', 'morning', 'just', 'a', 'reminder', 'to', 'get', 'your', 'aga', 'estimates', 'in', 'by', 'noon', 'est', 'cst', 'today', 'last', 'year', 'last', 'week', 'thank', 'you', 'george', 'ellis', 'bnp', 'paribas', 'commodity', 'futures', 'inc', 'ce', 'message', 'et', 'toutes', 'les', 'pieces', 'jointes', 'ci', 'apres', 'le', 'message', 'sont', 'etablis', 'a', 'l', 'intention', 'exclusive', 'de', 'ses', 'destinataires', 'et', 'sont', 'confidentiels', 'si', 'vous', 'recevez', 'ce', 'message', 'par', 'erreur', 'merci', 'de', 'le', 'detruire', 'et', 'd', 'en', 'avertir', 'immediatement', 'l', 'expediteur', 'toute', 'utilisation', 'de', 'ce', 'message', 'non', 'conforme', 'a', 'sa', 'destination', 'toute', 'diffusion', 'ou', 'toute', 'publication', 'totale', 'ou', 'partielle', 'est', 'interdite', 'sauf', 'autorisation', 'expresse', 'l', 'internet', 'ne', 'permettant', 'pas', 'd', 'assurer', 'l', 'integrite', 'de', 'ce', 'message', 'bnp', 'paribas', 'et', 'ses', 'filiales', 'decline', 'nt', 'toute', 'responsabilite', 'au', 'titre', 'de', 'ce', 'message', 'dans', 'l', 'hypothese', 'ou', 'il', 'aurait', 'ete', 'modifie', 'this', 'message', 'and', 'any', 'attachments', 'the', 'message', 'are', 'intended', 'solely', 'for', 'the', 'addressees', 'and', 'are', 'confidential', 'if', 'you', 'receive', 'this', 'message', 'in', 'error', 'please', 'delete', 'it', 'and', 'immediately', 'notify', 'the', 'sender', 'any', 'use', 'not', 'in', 'accord', 'with', 'its', 'purpose', 'any', 'dissemination', 'or', 'disclosure', 'either', 'whole', 'or', 'partial', 'is', 'prohibited', 'except', 'formal', 'approval', 'the', 'internet', 'can', 'not', 'guarantee', 'the', 'integrity', 'of', 'this', 'message', 'bnp', 'paribas', 'and', 'its', 'subsidiaries', 'shall', 'will', 'not', 'therefore', 'be', 'liable', 'for', 'the', 'message', 'if', 'modified']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for key, value in islice(doc.items(), 2):\n",
        "    print(key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2yVi76Ij1ys"
      },
      "source": [
        "## Dictionary Size (5 points)\n",
        "\n",
        "Next you should report the size of your dictionary, that is, how many unique tokens among all the documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53bAA66zkI55",
        "outputId": "2b23bfc1-8b38-4cab-db11-8e307a4a812c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of Unique Tokens: 9410\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127149"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "print(\"No.of Unique Tokens:\",len(set(tokens)))\n",
        "len(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU928IKKkJGY"
      },
      "source": [
        "## Top-20 Words (5 points)\n",
        "\n",
        "Finally, you should print a list of the top-20 most popular words by counting among all documents.\n",
        "\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "* Rank. Token, Count\n",
        "\n",
        "1. awesome, 20\n",
        "2. cool, 15\n",
        "3. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8MS6n6oNklvf",
        "outputId": "61f7b87e-9e07-408d-bc8c-6bf0bc604120"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 4111),\n",
              " ('to', 3768),\n",
              " ('ect', 2600),\n",
              " ('enron', 2296),\n",
              " ('a', 1897),\n",
              " ('and', 1795),\n",
              " ('you', 1749),\n",
              " ('of', 1716),\n",
              " ('i', 1696),\n",
              " ('on', 1590),\n",
              " ('john', 1514),\n",
              " ('in', 1465),\n",
              " ('hou', 1345),\n",
              " ('com', 1319),\n",
              " ('is', 1202),\n",
              " ('for', 1165),\n",
              " ('arnold', 999),\n",
              " ('subject', 936),\n",
              " ('s', 871),\n",
              " ('it', 865)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter(tokens)\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_word_counts[:20]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcrFD2F-N-Jp"
      },
      "source": [
        "# Part 2: Boolean Retrieval (30 points)\n",
        "\n",
        "In this part you will build an inverted index to support Boolean retrieval. You should use the tokenization strategy from above.\n",
        "\n",
        "We require your index to support AND, OR, NOT queries.\n",
        "\n",
        "Search for the queries below using your index and print out matching documents (for each query, print out 5 matching documents):\n",
        "* buyer\n",
        "* margins AND limits\n",
        "* winter OR summer\n",
        "* buyers AND risk AND NOT crazy\n",
        "* never OR know\n",
        "\n",
        "Recall, that you should apply the exact same pre-processing strategies to the query as we do to the documents.\n",
        "\n",
        "The output should like this:\n",
        "* DocumentID Document\n",
        "\n",
        "To make our life easier, please output the DocumentIDs in lexicographic order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GS3Tc_kYN-Jp"
      },
      "outputs": [],
      "source": [
        "# build the index here\n",
        "# add cells as needed to organize your code\n",
        "\n",
        "##Building inverted Index\n",
        "inv_index={}\n",
        "for key, tokens in doc.items():\n",
        "  for token in tokens:\n",
        "    if token not in inv_index:\n",
        "      inv_index[token]=[]\n",
        "    inv_index[token].append(key)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MOnEDvalb7al"
      },
      "outputs": [],
      "source": [
        "# def retrieve_docs_only(query):\n",
        "#   query=query.split()\n",
        "#   print(query)\n",
        "#   retrieved_docs=[]\n",
        "#   for i in range(len(query)):\n",
        "#     # print(retrieved_docs)\n",
        "#     if i%2==0:\n",
        "#       if query[i] not in inv_index:\n",
        "#         return []\n",
        "#       else:\n",
        "#         retrieved_docs.append(inv_index[query[i]])\n",
        "#     # elif i%2==1:\n",
        "#     if query[i]=='NOT':\n",
        "#       retrieved_docs.append(list(set(doc.keys())-set(inv_index[query[i]])))\n",
        "#       del query[i]\n",
        "#       print(query)\n",
        "#   return retrieved_docs\n",
        "# def perform_operations(query, retrieved_docs):\n",
        "#   if type(query)== str:\n",
        "#     query=query.split()\n",
        "#   if len(query)<=1:\n",
        "#     return retrieved_docs\n",
        "#   # print(query)\n",
        "#   if query[1]== 'OR':\n",
        "#     retrieved_docs[0]=set(retrieved_docs[0]+retrieved_docs[1])\n",
        "#   elif query[1]== 'AND':\n",
        "#     # print(retrieved_docs)\n",
        "#     retrieved_docs[0]=list(set(retrieved_docs[0]).intersection(retrieved_docs[1]))\n",
        "#   # elif query[1]== 'NOT':\n",
        "#   #   retrieved_docs[0]=list(set(retrieved_docs[0])-set(retrieved_docs[1]))\n",
        "#   # return retrieved_docs[0]\n",
        "#   if len(retrieved_docs) >= 2:del retrieved_docs[1]\n",
        "#   del query[:3]\n",
        "#   # print(retrieved_docs)\n",
        "#   return perform_operations(query, retrieved_docs)\n",
        "# query=\"buyers AND risk AND NOT crazy\"\n",
        "# retrieved_docs=retrieve_docs_only(query)\n",
        "# # print(retrieved_docs)\n",
        "# r_doc_id = list(perform_operations(query, retrieved_docs)[0])\n",
        "# print(r_doc_id)\n",
        "# for doc_id in r_doc_id:\n",
        "#   print(doc_id, lookup_doc[doc_id])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB-keUMS6vkc"
      },
      "outputs": [],
      "source": [
        "# print(lookup_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82YUVPUknOjX"
      },
      "source": [
        "## Running the five queries (4 points each, 20 points in total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qWH8h5ZkN-Jp"
      },
      "outputs": [],
      "source": [
        "def preprocess_query(query):\n",
        "    return query.lower().split()\n",
        "\n",
        "def and_query(inv_index, terms):\n",
        "    if not terms:\n",
        "        return []\n",
        "    processed_result = set(inv_index.get(terms[0], []))\n",
        "    for term in terms[1:]:\n",
        "        processed_result &= set(inv_index.get(term, []))\n",
        "    return list(processed_result)\n",
        "\n",
        "def or_query(inv_index, terms):\n",
        "    processed_result = set()\n",
        "    for term in terms:\n",
        "        processed_result |= set(inv_index.get(term, []))\n",
        "    return list(processed_result)\n",
        "\n",
        "def not_query(inv_index, term, all_docs):\n",
        "    docs_with_term = set(inv_index.get(term, []))\n",
        "    return list(all_docs - docs_with_term)\n",
        "\n",
        "def parse_query(inv_index, query, all_docs):\n",
        "    terms = preprocess_query(query)\n",
        "    if not terms:\n",
        "        return list(all_docs)\n",
        "    i = 0\n",
        "    processed_terms = []\n",
        "    while i < len(terms):\n",
        "        if terms[i] == \"not\" and i + 1 < len(terms):\n",
        "            not_processed_result = set(not_query(inv_index, terms[i + 1], all_docs))\n",
        "            processed_terms.append((\"TERM\", not_processed_result))\n",
        "            i += 2\n",
        "        else:\n",
        "            if terms[i] not in (\"and\", \"or\"):\n",
        "                term_docs = set(inv_index.get(terms[i], []))\n",
        "                processed_terms.append((\"TERM\", term_docs))\n",
        "            else:\n",
        "                processed_terms.append((\"OP\", terms[i]))\n",
        "            i += 1\n",
        "    i = 0\n",
        "    and_processed = []\n",
        "    while i < len(processed_terms):\n",
        "        if i == 0 and processed_terms[i][0] != \"OP\":\n",
        "            and_processed.append((\"TERM\", processed_terms[i][1]))\n",
        "            i += 1\n",
        "        elif i+1 < len(terms) and processed_terms[i][1] == \"and\":\n",
        "            left_type, left_docs   = and_processed[-1]\n",
        "            right_type, right_docs = processed_terms[i+1]\n",
        "            if left_type in (\"TERM\") and right_type in (\"TERM\"):\n",
        "                and_processed_result        = left_docs & right_docs\n",
        "                and_processed[-1] = (\"TERM\", and_processed_result)\n",
        "                i += 2\n",
        "            else:\n",
        "                and_processed.append(processed_terms[i])\n",
        "                i += 1\n",
        "        else:\n",
        "            and_processed.append(processed_terms[i])\n",
        "            i += 1\n",
        "    i = 0\n",
        "    final_processed = []\n",
        "    while i < len(and_processed):\n",
        "        if i == 0 and and_processed[i][0] != \"OP\":\n",
        "            final_processed.append((\"TERM\", and_processed[i][1]))\n",
        "            i += 1\n",
        "        elif i+1 < len(terms) and and_processed[i][1] == \"or\":\n",
        "            left_type, left_docs   = final_processed[-1]\n",
        "            right_type, right_docs = and_processed[i+1]\n",
        "            if left_type in (\"TERM\") and right_type in (\"TERM\"):\n",
        "                or_processed_result           = left_docs | right_docs\n",
        "                final_processed[-1] = (\"TERM\", or_processed_result)\n",
        "                i += 2\n",
        "            else:\n",
        "                final_processed.append(and_processed[i])\n",
        "                i += 1\n",
        "        else:\n",
        "            final_processed.append(and_processed[i])\n",
        "            i += 1\n",
        "    return list(set(final_processed[-1][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzx6tpmpjBsX"
      },
      "source": [
        "Now show the results for the query: `buyer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gUOUjF18jFTT",
        "outputId": "af6e16ed-03ad-40e2-a8b1-6312a6472c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of results: 9\n",
            "13960264.1075857658698 \n",
            " very useful...thx.   keep me posted\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Caroline Abramo@ENRON\n",
            "12/22/2000 11:41 AM\n",
            "To: John Arnold/HOU/ECT@ECT, Mike Maggi/Corp/Enron@Enron, Jennifer \n",
            "Fraser/HOU/ECT@ECT\n",
            "cc: Per Sekse/NY/ECT@ECT \n",
            "Subject: fund views\n",
            "\n",
            "Hi- all the funds are trying to figure out what the play is for next year- \n",
            "major divergence of opinions.  Most everyone we talk to takes a macro view.\n",
            "\n",
            "+ Dwight Anderson from Tudor thinks anything above $6 is a sale from the \n",
            "perspective of shut in industrial demand- he believes tha\n",
            "\n",
            " ----------------------\n",
            "17195279.1075857655281 \n",
            " i think this is the biggest chopfest in nat gas history.  scale up seller, \n",
            "scale down buyer.\n",
            "reviewing my aga model and assumptions later today.  i'll see if i have any \n",
            "new inspirations.  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slafontaine@globalp.com on 03/02/2001 10:25:45 AM\n",
            "To: John.Arnold@enron.com\n",
            "cc:  \n",
            "Subject: Re: silverman\n",
            "\n",
            "\n",
            "\n",
            "he will be if we cut him off for a week i bet he gets some inspiration. have a\n",
            "good weekdn. any view here? i think short term range stuff-med-longer term you\n",
            "know what i think.\n",
            "  sprds/front to bac\n",
            "\n",
            " ----------------------\n",
            "2268604.1075857652949 \n",
            " fyi : bo is a big put buyer and fence seller today.  though he is trying to \n",
            "defend j.  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slafontaine@globalp.com on 03/29/2001 09:22:15 PM\n",
            "To: John.Arnold@enron.com\n",
            "cc:  \n",
            "Subject: Re: power gen\n",
            "\n",
            "\n",
            "\n",
            "agree on view. as u cud tell i got a little less bearish for a bit so i delta\n",
            "hedged and day traded to keep from losing a ton, let go og the delta after aga\n",
            "number which was exactly on my forecast still implying 7.5 bcf swing(i \n",
            "actually\n",
            "thot it could have been worse so my range pre was 5-12 basis \n",
            "\n",
            " ----------------------\n",
            "25732708.1075857656969 \n",
            " don't care about the front.  i think its vulnerable to a good short squeeze \n",
            "like we saw on thruday and friday.  trade is getting short in here with cash \n",
            "such a piece.  if weather ever changes, which the weather boys are saying it \n",
            "might in 2 weeks, the cash players are going to be big buyers.  don't really \n",
            "want to carry length on the way down waiting for that to happen though.   \n",
            "Backs are crazy stong.  cal 3 traded as high as +10.  everybody a buyer as \n",
            "california trying to buy any fixed pri\n",
            "\n",
            " ----------------------\n",
            "28376645.1075857655238 \n",
            " I think you need to check your AGA model.  7.2 bcf/d seems awfully high.  \n",
            "That's saying that had gas been at $2.5, the AGA for the week would have been \n",
            "151.  Look at the AGA history for last week, this week and next week.\n",
            "\n",
            "   94 95 96 97 98 99 00 01 \n",
            "3rd week of Feb  -64 -46 -64 -63 -77 -97 -136 -81\n",
            "4th week of Feb  -132 -118 -62 -76 -47 -128 -74 -101 \n",
            "1st week of Mar  -27 -132 -118 -57 -54 -69 -37 ??\n",
            "\n",
            "Do you really think the # would have been 151 with no demand destruction?  It \n",
            "wasn't even r\n",
            "\n",
            " ----------------------\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"buyer\"\n",
        "id_retrievals=parse_query(inv_index, query, set(doc.keys()))\n",
        "print(\"No.of results:\", len(id_retrievals))\n",
        "\n",
        "id_retrievals=sorted(list(id_retrievals))[:5]\n",
        "for doc_id in id_retrievals:\n",
        "  print(doc_id, \"\\n\",lookup_doc[doc_id][:500])\n",
        "  # print(id_retrievals)\n",
        "  # print(doc_id, \"\\n\",lookup_doc[doc_id][:500]\n",
        "  print(\"\\n ----------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbJ6yn3yMQWY"
      },
      "source": [
        "*Now* show the results for the query: `margins AND limits`\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SmF62rQ_MRlO",
        "outputId": "8cad6a8f-d052-4349-8fa5-a34ee3c4276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of results: 1\n",
            "33025919.1075857594206 \n",
            " saw a lot of the bulls sell summer against length in front to mitigate \n",
            "margins/absolute position limits/var.  as these guys are taking off the \n",
            "front, they are also buying back summer.  el paso large buyer of next winter \n",
            "today taking off spreads.  certainly a reason why the spreads were so strong \n",
            "on the way up and such a piece now.   really the only one left with any risk \n",
            "premium built in is h/j now.   it was trading equivalent of 180 on access, \n",
            "down 40+ from this morning.  certainly if we \n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"margins AND limits\"\n",
        "id_retrievals=parse_query(inv_index, query, set(doc.keys()))\n",
        "print(\"No.of results:\", len(id_retrievals))\n",
        "\n",
        "id_retrievals=sorted(list(id_retrievals))[:5]\n",
        "for doc_id in id_retrievals:\n",
        "  print(doc_id, \"\\n\",lookup_doc[doc_id][:500])\n",
        "  # print(id_retrievals)\n",
        "  # print(doc_id, \"\\n\",lookup_doc[doc_id][:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KlaH0fMTO5"
      },
      "source": [
        "Now show the results for the query: `winter OR summer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MAFBtPGmMVRW",
        "outputId": "2b0cf5c9-83d9-4a4d-bc3b-ddf0585d87e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of results: 58\n",
            "10353423.1075857652669 \n",
            " maybe.  hydro situation dire in west.  think water levels are at recent \n",
            "historical lows.  problem is from gas standpoint, west is an island right \n",
            "now.  every molecle that can go there is.  so will provide limited support to \n",
            "prices in east.  hydro in east is actually very healthy.  would assume your \n",
            "markets are targeting eastern u.s. so i dont know if hydro problem in west is \n",
            "that relevant.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sarah Mulholland\n",
            "04/04/2001 08:09 AM\n",
            "To: John Arnold/HOU/ECT@ECT\n",
            "cc:  \n",
            "Subject: Re: us fuel 4/2/01\n",
            "10537445.1075857655390 \n",
            " industrial demand the scary thing.  no question there are some steel mills \n",
            "and auto factories and plastics plants that were on last november that arent \n",
            "coming up now and its not due to gas prices.  the economy sucks and it will \n",
            "affect ind demand.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slafontaine@globalp.com on 02/28/2001 08:03:43 AM\n",
            "To: John.Arnold@enron.com\n",
            "cc:  \n",
            "Subject: Re: mkts\n",
            "\n",
            "\n",
            "\n",
            "at least a myn dollars-need to talk to pira on that. excellant point. need to \n",
            "do\n",
            "some margin analyses . having said all that look at corporate\n",
            "10603457.1075857597605 \n",
            " Thank you for sending this.  If only you sent it a couple hours earlier.   \n",
            "Just kidding.  T Boone must have heard this because he sold everything \n",
            "today.   9000 contracts.\n",
            "John\n",
            "\n",
            "\n",
            "   \n",
            "\t\n",
            "\t\n",
            "\tFrom:  Jennifer Fraser                           09/28/2000 03:35 PM\n",
            "\t\n",
            "\n",
            "To: John Arnold/HOU/ECT@ECT\n",
            "cc:  \n",
            "Subject: PIRA Annual Seminar Preview\n",
            "\n",
            "Hey JA:\n",
            "I was at PIRA today and got a preview of their presenation on Oct 11-12 for  \n",
            "their client seminars.\n",
            "(Greg Shuttlesworth)\n",
            "Summary:\n",
            "They have turned a little be\n",
            "11971129.1075857654956 \n",
            " theres only one thing i can think of...   storage field turning around gives \n",
            "cash market completely different feel.   instead of utilities looking to sell \n",
            "gas everday, they look to buy it.  huge difference in feel of mrket.  not so \n",
            "much actual gas but completely different economics of how marginal mmbtu gets \n",
            "priced.  tightening cash market causes cash players to buy futures... hence \n",
            "the tendency for a spring rally every year.  read heffner today...even he \n",
            "talks about it\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slafontaine@glo\n",
            "12541953.1075857657123 \n",
            " Neal:\n",
            "Referencing Apr-Oct 5315/35 and Nov-Mar 534/536, on the following volumes I \n",
            "am 533/536.  Feel free to call to transact.  713-853-3230\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Wood, Neal\" <Neal.Wood@usa.conoco.com> on 02/05/2001 11:33:51 AM\n",
            "To: \"'john.arnold@enron.com'\" <john.arnold@enron.com>\n",
            "cc:  \n",
            "Subject: APR01-MAR02 Strip, varying monthly volumes\n",
            "\n",
            "\n",
            "John,\n",
            "\n",
            "I am interested in purchasing the following APR01-MAR02 NYMEX strip:\n",
            "\n",
            "APR 2001 US Gas Swap NYMEX   86,650 MMBtu per month\n",
            "MAY 2001 US Gas Swap NYMEX   69,018\n",
            "JUN 2001 U\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"winter OR summer\"\n",
        "id_retrievals=parse_query(inv_index, query, set(doc.keys()))\n",
        "print(\"No.of results:\", len(id_retrievals))\n",
        "\n",
        "id_retrievals=sorted(list(id_retrievals))[:5]\n",
        "for doc_id in id_retrievals:\n",
        "  print(doc_id, \"\\n\",lookup_doc[doc_id][:500])\n",
        "  # print(id_retrievals)\n",
        "  # print(doc_id, \"\\n\",lookup_doc[doc_id][:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVcjjsBWIFv"
      },
      "source": [
        "Now show the results for the query: `buyers AND risk AND NOT crazy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e7Ipk2JWIF5",
        "outputId": "1e8ce025-4fb8-4822-ac09-4c0bf5fda63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of results: 1\n",
            "2726985.1075857655016 \n",
            " ---------------------- Forwarded by John Arnold/HOU/ECT on 03/06/2001 07:48 \n",
            "AM ---------------------------\n",
            "From: Ann M Schmidt@ENRON on 03/05/2001 08:23 AM\n",
            "To: Ann M Schmidt/Corp/Enron@ENRON\n",
            "cc:  (bcc: John Arnold/HOU/ECT)\n",
            "Subject: Enron Mentions - 03-04-01\n",
            "\n",
            "Utility Deregulation: Square Peg, Round Hole?\n",
            "The New York Times, 03/04/01\n",
            "\n",
            "3 Executives Considered to Head Military\n",
            "Los Angeles Times, 03/04/01\n",
            "\n",
            "Bush leaning toward execs for military\n",
            "The Seattle Times, 03/04/01\n",
            "\n",
            "Enron's Chief Denies Role \n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"buyers AND risk AND NOT crazy\"\n",
        "id_retrievals=parse_query(inv_index, query, set(doc.keys()))\n",
        "print(\"No.of results:\", len(id_retrievals))\n",
        "\n",
        "id_retrievals=sorted(list(id_retrievals))[:5]\n",
        "for doc_id in id_retrievals:\n",
        "  print(doc_id, \"\\n\",lookup_doc[doc_id][:500])\n",
        "  # print(id_retrievals)\n",
        "  # print(doc_id, \"\\n\",lookup_doc[doc_id][:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ0sd8zrWIbb"
      },
      "source": [
        "Now show the results for the query: `never OR know`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g2BumYswWIbc",
        "outputId": "a11a2a20-1243-4d70-fcf4-9873705db91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of results: 168\n",
            "10008095.1075857595829 \n",
            " not me\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Brian Hoskins@ENRON COMMUNICATIONS\n",
            "10/23/2000 03:47 PM\n",
            "To: John Arnold/HOU/ECT@ECT\n",
            "cc:  \n",
            "Subject: Re:\n",
            "\n",
            "Never mind.  Did you do that?\n",
            "\n",
            "\n",
            "Brian T. Hoskins\n",
            "Enron Broadband Services\n",
            "713-853-0380 (office)\n",
            "713-412-3667 (mobile)\n",
            "713-646-5745 (fax)\n",
            "Brian_Hoskins@enron.net\n",
            "\n",
            "\n",
            "----- Forwarded by Brian Hoskins/Enron Communications on 10/23/00 03:54 PM \n",
            "-----\n",
            "\n",
            "\tJohn Cheng@ENRON\n",
            "\tSent by: John Cheng@ENRON\n",
            "\t10/23/00 03:47 PM\n",
            "\t\t\n",
            "\t\t To: John Cheng/NA/Enron@Enron\n",
            "\t\t cc: Brian Hoskins/Enron Communication\n",
            "10216181.1075857658324 \n",
            " I want John to interview with the various desk heads (Scott, Hunter, Phillip, \n",
            "Tom).  I think I'm going to tell John not to mention the past.   It's an \n",
            "issue that doesn't need to be made public and as long as Lavo and myself are \n",
            "okay with it, I don't see the need to get individual approval from everyone.  \n",
            "Thanks for your help,\n",
            "John\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ed McMichael\n",
            "01/03/2001 08:37 AM\n",
            "To: John Arnold/HOU/ECT@ECT\n",
            "cc:  \n",
            "Subject: Re:  \n",
            "\n",
            "Thanks for the inquiry.  I sincerely appreciate you giving me the heads up. \n",
            "10337786.1075857649963 \n",
            " For extra credit.... \n",
            "If the company is worth 150% more under management A rather than 50% more, \n",
            "does your answer change?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Eva Pao\" <epao@mba2002.hbs.edu> on 05/11/2001 05:13:59 PM\n",
            "Please respond to <epao@mba2002.hbs.edu>\n",
            "To: <John.Arnold@enron.com>\n",
            "cc:  \n",
            "Subject: RE: try this one...\n",
            "\n",
            "\n",
            "will you do all of my homework?\n",
            "\n",
            "-----Original Message-----\n",
            "From: John.Arnold@enron.com [mailto:John.Arnold@enron.com]\n",
            "Sent: Friday, May 11, 2001 8:41 AM\n",
            "To: epao@mba2002.hbs.edu\n",
            "Subject: Re: try this one...\n",
            "\n",
            "10353423.1075857652669 \n",
            " maybe.  hydro situation dire in west.  think water levels are at recent \n",
            "historical lows.  problem is from gas standpoint, west is an island right \n",
            "now.  every molecle that can go there is.  so will provide limited support to \n",
            "prices in east.  hydro in east is actually very healthy.  would assume your \n",
            "markets are targeting eastern u.s. so i dont know if hydro problem in west is \n",
            "that relevant.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sarah Mulholland\n",
            "04/04/2001 08:09 AM\n",
            "To: John Arnold/HOU/ECT@ECT\n",
            "cc:  \n",
            "Subject: Re: us fuel 4/2/01\n",
            "10396784.1075857651750 \n",
            " fuck you\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "239b3989d@msn.com on 04/14/2001 05:42:02 AM\n",
            "Please respond to leehouse211@asia.com\n",
            "To: 6na10@msn.com\n",
            "cc:  \n",
            "Subject: I need your phone # to help your debt \n",
            "problem.                                                   [h7gmu]\n",
            "\n",
            "\n",
            "How would you like to take all of your debt, reduce\n",
            "or eliminate the interest, pay less per month,and\n",
            "pay them off sooner?\n",
            "\n",
            "We have helped over 20,000 people do just that.\n",
            "\n",
            "If you are interested, we invite you request our free\n",
            "information by provide the following\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query=\"never OR know\"\n",
        "id_retrievals=parse_query(inv_index, query, set(doc.keys()))\n",
        "print(\"No.of results:\", len(id_retrievals))\n",
        "\n",
        "id_retrievals=sorted(list(id_retrievals))[:5]\n",
        "for doc_id in id_retrievals:\n",
        "  print(doc_id, \"\\n\",lookup_doc[doc_id][:500])\n",
        "  # print(id_retrievals)\n",
        "  # print(doc_id, \"\\n\",lookup_doc[doc_id][:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAhHuIDfN-Jp"
      },
      "source": [
        "## Observations (10 points)\n",
        "Does your boolean search engine find relevant documents for these queries? As in, would our customers be happy if we shipped this retrieval engine? Why or why not?\n",
        "\n",
        "What is the impact of the pre-processing options? Do they impact your search quality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX6k-Vil0GIy"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "#### The search engine is fetching relevant documents for the queries. The customers will be happy as it makes their search easy and the search engine has the capability for long queries. So, the user can modify their search for exact results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5JYoNNrlimy"
      },
      "source": [
        "# Part 3: Ranking (35 points)\n",
        "\n",
        "For the third part, you will add ranking to your search system. Given a search query, our goal is to retrieve the top-5 most relevant emails by assigning a score to each document.\n",
        "\n",
        "We will explore two ranking methods, each offering a different approach to scoring and ranking documents:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMlmJ-8mP8X"
      },
      "source": [
        "### Ranking method A: Ranking with vector space model with TF-IDF (15 points)\n",
        "\n",
        "**Cosine:** You should use cosine as your scoring function.\n",
        "\n",
        "**TFIDF:** For the **document vectors**, use the standard TF-IDF scores introduced in class. For the **query vector**, use **simple weights (the raw term frequency)**. For example:\n",
        "* query: never $\\rightarrow$ (1)\n",
        "* query: never know $\\rightarrow$ (1, 1)\n",
        "\n",
        "**Query:**  `trade`\n",
        "\n",
        "**Output:**\n",
        "You should output the top-5 results plus the cosine score of each of these documents.  \n",
        "\n",
        "The output should be like this:\n",
        "\n",
        "Rank Scores DocumentID Document\n",
        "\n",
        "---\n",
        "\n",
        "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OMOP3bomAM2",
        "outputId": "604924a7-7176-4e64-a3ab-ac08a6f61980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'trade': 1}\n",
            "Rank: 1 cosine_score: 0.35268279961678006 document_id: 32835197.1075857597302 \n",
            " Hey:\n",
            "I just want to confirm the trades I have in your book.\n",
            "Trade #1.  I sell 4000 X @ 4652\n",
            "\n",
            "Trade #2. I buy 4000 X @ 4652\n",
            "  I sell 4000 X @ 4902\n",
            "\n",
            "Trade #3 I buy 4000 X @ 5000\n",
            "  I sell 4000 F @ 5000\n",
            "\n",
            "\n",
            "Net result: I have 4000 F in your book @ 4902.\n",
            "Thanks, \n",
            "John\n",
            "\n",
            "-------------------------------------------------\n",
            "Rank: 2 cosine_score: 0.3053426239418323 document_id: 15827855.1075857658654 \n",
            " torrey:\n",
            "please set me up to trade crude.\n",
            "John\n",
            "\n",
            "-------------------------------------------------\n",
            "Rank: 3 cosine_score: 0.2283779396714838 document_id: 2752057.1075857658632 \n",
            " Torrey:\n",
            "Can you also approve Mike Maggi to trade crude as well.  Thanks for your help.\n",
            "John\n",
            "\n",
            "-------------------------------------------------\n",
            "Rank: 4 cosine_score: 0.22160785591147464 document_id: 3383202.1075857656796 \n",
            " you fucker that's my trade.   i was trying to buy nines the last 20 minutes.  \n",
            "all i got was scraps.  50-100.  i think it's a great trade.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "slafontaine@globalp.com on 02/07/2001 01:41:44 PM\n",
            "To: John.Arnold@enron.com\n",
            "cc:  \n",
            "Subject: Re: weather pop\n",
            "\n",
            "\n",
            "\n",
            "that is nuts-good sale-im gonna sell jun or july otm calls at some point\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------\n",
            "Rank: 5 cosine_score: 0.1852881258960075 document_id: 5340834.1075857658345 \n",
            " greg:\n",
            "what is the (correct) formula you devised for profitability on last trade is \n",
            "mid?\n",
            "\n",
            "-------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# print(word_counts)\n",
        "from collections import defaultdict\n",
        "import math\n",
        "# for key, value in islice(doc.items(), 2):\n",
        "#     print(key, value)\n",
        "\n",
        "def tfidf(doc):\n",
        "  doc_feq=defaultdict(int)\n",
        "  tfidf_scores={}\n",
        "  for tks in doc.values():\n",
        "    for token in set(tks):\n",
        "      doc_feq[token]+=1\n",
        "  for doc_id, tks in doc.items():\n",
        "      tfidf_vector={}\n",
        "      total_tks=len(tks)\n",
        "      for t in tks:\n",
        "        tf=tks.count(t)/total_tks\n",
        "        idf=math.log(len(doc)/doc_feq[t])\n",
        "        tfidf_vector[t]=tf*idf\n",
        "      # doc[doc_id]=tfidf_vector\n",
        "      tfidf_scores[doc_id]=tfidf_vector\n",
        "  return tfidf_scores\n",
        "\n",
        "def cosine_similarity(query_vector, doc_vector):\n",
        "    common_keys=set(query_vector.keys()).intersection(set(doc_vector.keys()))\n",
        "    # for key in common_keys:\n",
        "    dot_product=sum(query_vector[key]*doc_vector[key] for key in common_keys)\n",
        "    query_magnitude=math.sqrt(sum(query_vector[key]**2 for key in query_vector))\n",
        "    doc_magnitude=math.sqrt(sum(doc_vector[key]**2 for key in doc_vector))\n",
        "    if query_magnitude==0 or doc_magnitude==0:\n",
        "      return 0\n",
        "    return dot_product/(query_magnitude*doc_magnitude)\n",
        "\n",
        "tfidf_scores=tfidf(doc)\n",
        "# print(tfidf_scores)\n",
        "query=\"trade\"\n",
        "query_vector={}\n",
        "query_token=custom_tokenize(query)\n",
        "for key in query_token:\n",
        "  query_vector[key]=1\n",
        "print(query_vector)\n",
        "scores={}\n",
        "for doc_id, doc_vector in tfidf_scores.items():\n",
        "  scores[doc_id]=cosine_similarity(query_vector, doc_vector)\n",
        "ranked_results=sorted(scores.items(), key=lambda x: (x[1],x[0]), reverse=True)[:5]\n",
        "# print(ranked_results)\n",
        "for i in range(len(ranked_results)):\n",
        "  print(\"Rank:\",i+1, \"cosine_score:\",ranked_results[i][1], \"document_id:\",ranked_results[i][0], \"\\n\", lookup_doc[ranked_results[i][0]])\n",
        "  print(\"\\n-------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print out the top-5 retrieved emails\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf3-HsRamBkh"
      },
      "source": [
        "### Ranking method B: Ranking with BM25 (15 points)\n",
        "Finally, let's try the BM25 approach for ranking. Refer to https://en.wikipedia.org/wiki/Okapi_BM25 for the specific formula. You could choose k_1 = 1.2 and b = 0.75 but feel free to try other options.\n",
        "\n",
        "**Query:**  `gas floor`\n",
        "\n",
        "**Output:**\n",
        "You should output the top-5 results plus the BM25 score of each of these documents.  \n",
        "\n",
        "The output should be like this:\n",
        "\n",
        "Rank Scores DocumentID Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lESwxZNImIle",
        "outputId": "2442462e-fbaa-48bb-e710-b9912a99ccad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1, Document ID: 29559946.1075857598198, BM25 Score: 7.6600, Content: ['thx', 'for', 'the', 'spreadsheet', 'questions', 'what', 'time', 'frame', 'does', 'this', 'entail', 'and', 'does', 'the', 'correlation', 'between', 'the', 'trader', 'and', 'agg', 'gas', 'include', 'that', 'trader', 's', 'contribution', 'to', 'the', 'floor', 's', 'p', 'l', 'in', 'other', 'words', 'is', 'my', 'p', 'l', 'correlated', 'with', 'the', 'floor', 'or', 'is', 'it', 'correlated', 'to', 'the', 'rest', 'of', 'the', 'floor', 'absent', 'me', 'enron', 'north', 'america', 'corp', 'from', 'frank', 'hayden', 'enron', 'pm', 'to', 'john', 'arnold', 'hou', 'ect', 'ect', 'cc', 'subject']\n",
            "\n",
            "---------------------------------------------------------\n",
            "Rank: 2, Document ID: 32732331.1075857597410, BM25 Score: 6.8492, Content: ['john', 'i', 'have', 'asked', 'mike', 'and', 'larry', 'to', 'spend', 'half', 'an', 'hour', 'each', 'talking', 'to', 'you', 'about', 'opportunities', 'on', 'the', 'gas', 'floor', 'please', 'advise', 'if', 'the', 'following', 'schedule', 'is', 'unacceptable', 'i', 'will', 'be', 'leaving', 'today', 'at', 'larry', 'mike', 'thanks', 'john']\n",
            "\n",
            "---------------------------------------------------------\n",
            "Rank: 3, Document ID: 23846275.1075857658302, BM25 Score: 6.8236, Content: ['john', 'i', 'would', 'like', 'for', 'you', 'to', 'come', 'talk', 'to', 'a', 'couple', 'more', 'people', 'on', 'the', 'gas', 'floor', 'about', 'a', 'possible', 'position', 'down', 'the', 'road', 'my', 'assistant', 'ina', 'rangle', 'is', 'going', 'to', 'schedule', 'a', 'couple', 'interviews', 'please', 'coordinate', 'with', 'her', 'john']\n",
            "\n",
            "---------------------------------------------------------\n",
            "Rank: 4, Document ID: 8915800.1075857597866, BM25 Score: 6.4081, Content: ['thanks', 'this', 'is', 'exactly', 'what', 'i', 'wanted', 'john', 'enron', 'north', 'america', 'corp', 'from', 'sunil', 'dalal', 'enron', 'pm', 'to', 'john', 'arnold', 'hou', 'ect', 'ect', 'cc', 'frank', 'hayden', 'corp', 'enron', 'enron', 'subject', 're', 'john', 'the', 'matrix', 'that', 'was', 'sent', 'to', 'you', 'includes', 'ytd', 'p', 'l', 'for', 'all', 'the', 'traders', 'in', 'the', 'matrix', 'trader', 'p', 'l', 'contribution', 'to', 'desk', 'p', 'l', 'however', 'was', 'not', 'backed', 'out', 'on', 'that', 'particular', 'matrix', 'that', 'matrix', 'effectively', 'shows', 'one', 'trader', 's', 'correlation', 'to', 'all', 'others', 'what', 'is', 'does', 'not', 'show', 'is', 'one', 'trader', 's', 'p', 'l', 'to', 'the', 'desk', 'p', 'l', 'i', 'have', 'included', 'a', 'spreadsheet', 'with', 'each', 'trader', 's', 'p', 'l', 'backed', 'out', 'of', 'agg', 'gas', 'p', 'l', 'to', 'demonstrate', 'their', 'relationship', 'please', 'call', 'frank', 'or', 'myself', 'if', 'you', 'have', 'questions', 'thanks', 'from', 'frank', 'hayden', 'pm', 'to', 'sunil', 'dalal', 'corp', 'enron', 'enron', 'cc', 'subject', 're', 'questions', 'and', 'answers', 'forwarded', 'by', 'frank', 'hayden', 'corp', 'enron', 'on', 'pm', 'john', 'arnold', 'ect', 'pm', 'to', 'frank', 'hayden', 'corp', 'enron', 'enron', 'cc', 'subject', 're', 'thx', 'for', 'the', 'spreadsheet', 'questions', 'what', 'time', 'frame', 'does', 'this', 'entail', 'and', 'does', 'the', 'correlation', 'between', 'the', 'trader', 'and', 'agg', 'gas', 'include', 'that', 'trader', 's', 'contribution', 'to', 'the', 'floor', 's', 'p', 'l', 'in', 'other', 'words', 'is', 'my', 'p', 'l', 'correlated', 'with', 'the', 'floor', 'or', 'is', 'it', 'correlated', 'to', 'the', 'rest', 'of', 'the', 'floor', 'absent', 'me', 'enron', 'north', 'america', 'corp', 'from', 'frank', 'hayden', 'enron', 'pm', 'to', 'john', 'arnold', 'hou', 'ect', 'ect', 'cc', 'subject']\n",
            "\n",
            "---------------------------------------------------------\n",
            "Rank: 5, Document ID: 3417404.1075857651247, BM25 Score: 6.0906, Content: ['can', 'you', 'send', 'jean', 'a', 'list', 'of', 'her', 'seat', 'numbers', 'forwarded', 'by', 'john', 'arnold', 'hou', 'ect', 'on', 'pm', 'from', 'jean', 'mrha', 'enron', 'enronxgate', 'on', 'pm', 'to', 'john', 'arnold', 'hou', 'ect', 'ect', 'cc', 'subject', 'sixth', 'floor', 'layout', 'john', 'i', 'heard', 'from', 'wes', 'colwell', 'that', 'you', 'had', 'been', 'appointed', 'by', 'lavorato', 'to', 'layout', 'the', 'sixth', 'floor', 'for', 'gas', 'this', 'morning', 'i', 'spoke', 'to', 'wes', 'regarding', 'the', 'placement', 'of', 'the', 'upstream', 'ecommerce', 'desk', 'on', 'six', 'i', 'have', 'taken', 'spaces', 'but', 'i', 'need', 'two', 'more', 'the', 'two', 'i', 'would', 'like', 'to', 'use', 'e', 'e', 'are', 'currently', 'being', 'occupied', 'by', 'the', 'central', 'region', 'i', 'would', 'like', 'to', 'move', 'these', 'individuals', 'to', 'two', 'spots', 'right', 'across', 'from', 'their', 'current', 'location', 'e', 'e', 'for', 'your', 'information', 'the', 'current', 'six', 'spots', 'that', 'i', 'have', 'are', 'e', 'e', 'e', 'e', 'e', 'and', 'e', 'please', 'call', 'when', 'you', 'can', 'good', 'luck', 'trading', 'regards', 'mrha']\n",
            "\n",
            "---------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# Define the BM25 function\n",
        "def bm25(query, docs):\n",
        "    k1 = 1.2\n",
        "    b = 0.75\n",
        "    bm25_scores = {}\n",
        "    N = len(docs)\n",
        "    query_tokens = query.lower().split()\n",
        "    avgdl = sum(len(doc) for doc in docs.values()) / N  # Calculate the average document length\n",
        "\n",
        "    for doc_id, doc_tokens in docs.items():\n",
        "        doc_len = len(doc_tokens)\n",
        "        score = 0\n",
        "\n",
        "        for term in query_tokens:\n",
        "            doc_freq = sum(term in doc for doc in docs.values())\n",
        "            term_freq = doc_tokens.count(term)\n",
        "            idf = math.log((N - doc_freq + 0.5) / (doc_freq + 0.5) + 1)\n",
        "            numerator = term_freq * (k1 + 1)\n",
        "            denominator = term_freq + k1 * ((1 - b) + b * (doc_len / avgdl))\n",
        "            score += idf * numerator / denominator\n",
        "\n",
        "        bm25_scores[doc_id] = score\n",
        "\n",
        "    # Sort by score\n",
        "    sorted_docs = sorted(bm25_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "    return sorted_docs\n",
        "\n",
        "\n",
        "query = \"gas floor\"\n",
        "bm25_results = bm25(query, doc)\n",
        "\n",
        "# Print the results\n",
        "for i, (doc_id, score) in enumerate(bm25_results, start=1):\n",
        "    print(f\"Rank: {i}, Document ID: {doc_id}, BM25 Score: {score:.4f}, Content: {doc[doc_id]}\")\n",
        "    print(\"\\n---------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dScZI3PvnNqV"
      },
      "source": [
        "## Observations (5 points)\n",
        "What do you observe? Are there key differences between the two ranking approaches? Briefly discuss in bullet points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gku0X4oFrBRX"
      },
      "source": [
        "* Your observations:\n",
        "  - BM 25 uses advanced scoring mechanism when compared to TF-IDF\n",
        "  - BM 25 uses IDF, so we can say that BM25 is built upon TFIDF method\n",
        "  - TF-IDF is simpler and doesn't consider the document length\n",
        "* Differences:\n",
        "  - BM 25 doesn't use TF directly.\n",
        "  - BM 25 hyper-parameters can be tuned according to the usage.\n",
        "  - TFIDF is simpler, but doesn't perform well in complex scenarios. whereas, BM 25 is complex but performs good enough in complex scenarios too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwQKelDxgzS8"
      },
      "source": [
        "# Part 4: Cool LLM RAG Extension (15 points)\n",
        "\n",
        "Finally, we give you an opportunity to explore using OASIS for Retrieval-Augmented Generation (RAG) with LLMs.\n",
        "Here, the task is to retrieve the top-5 emails for a query you like. You will then pass the retrieved email content along with your question to the LLM and let it answer your question. Specifically, we want you:\n",
        "\n",
        "* Query the LLM directly with your question.\n",
        "* Retrieve the top-5 emails based on your query and pass them to the LLM along with your question.\n",
        "* How is the RAG output different from the non-RAG output? Does retrieval help the LLM better answer your question?\n",
        "\n",
        "We recommend using Gemini following the [instructions](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb) here.\n",
        "\n",
        "Hint: Take a close look at the dataset and pick a specific, relevant query where retrieval can enhance the LLM’s response.\n",
        "\n",
        "*What You Will Submit*\n",
        "- Your query.\n",
        "- The top-5 retrieved emails (including Document ID and ranking score).\n",
        "- The LLM's response without retrieval.\n",
        "- The LLM's response with retrieval (RAG).\n",
        "- A brief analysis comparing both outputs.\n",
        "\n",
        "We will grade this last part according to correctness, effort, and creativity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irxiT-iTgzS9"
      },
      "source": [
        "## Step 1: Query the LLM Directly (Without Retrieval) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzVP-WV9TTSx"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiyOf67igzS9"
      },
      "outputs": [],
      "source": [
        "# your code here to prompt LLM\n",
        "\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "hmbYB3E7TeYo",
        "outputId": "0eb15608-8951-40c0-c2a2-2e9e6fae80e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Enron was involved in a number of illegal activities that ultimately led to its downfall. These included:\n",
            "\n",
            "*   **Accounting fraud:** Enron used mark-to-market accounting to inflate its profits, and created special purpose entities (SPEs) to hide debt and losses.\n",
            "*   **Insider trading:** Enron executives sold their stock based on non-public information, while encouraging employees to buy and hold the stock.\n",
            "*   **Securities fraud:** Enron made false and misleading statements about its financial condition to investors.\n",
            "*   **Conspiracy:** Enron executives conspired to commit the above illegal activities.\n",
            "\n",
            "These activities were uncovered in 2001, leading to Enron's bankruptcy and the criminal prosecution of several executives.\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "query=\"Is there any illegal activities happening in enron\"\n",
        "response = model.generate_content(query)\n",
        "print(response.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otHscQFhgzTN"
      },
      "source": [
        "## Step 2: Query the LLM with Retrieved Emails (RAG) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "TnArY1PbgzTN",
        "outputId": "1497695a-a1e9-4ad8-b87b-70871fcf7215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This email exchange, taken in isolation, does not provide enough evidence to definitively conclude that illegal activities are taking place at Enron. However, it does suggest potentially questionable or concerning trading practices, and highlights a competitive environment where traders are focused on short-term gains and exploiting market inefficiencies.\n",
            "\n",
            "Here's a breakdown of the concerns and possible interpretations:\n",
            "\n",
            "*   **Spread Manipulation (Potentially Illegal):** The most concerning aspect is the explicit mention of \"blow[ing] out\" the H/J and J/K spreads. This refers to trying to artificially increase the spread difference by strategically selling J/K to pressure the price of J/K down, and, as a result, H/J would increase as well. This could be interpreted as an attempt to manipulate market prices for personal or company gain. The person even mentions being a seller of J/K so H/J *needs* to blow.\n",
            "*   **Focus on Short-Term Gains:** The emails reveal a strong focus on exploiting short-term price movements (\"a struggle each penny at this point\"). This can lead to high-risk trading strategies and potentially unethical behavior if traders prioritize immediate profits over fair market practices.\n",
            "*   **Scalable Selling and Price Manipulation:** The user mentions selling at smaller increments to allow markets to move but at a slower rate. This could also be seen as price manipulation.\n",
            "*   **Limited Context:** Without a deeper understanding of Enron's trading strategies, market conditions, and internal regulations, it's difficult to definitively determine if these activities were illegal or simply aggressive trading tactics.\n",
            "\n",
            "**In summary:**\n",
            "While these emails don't offer a smoking gun for illegal activities, they paint a picture of aggressive and potentially unethical trading practices focused on short-term gains and market manipulation. This behavior, especially the intentional \"blowing out\" of spreads, raises concerns about potential violations of fair market practices and regulations.\n"
          ]
        }
      ],
      "source": [
        "# print out the LLM response without the email content\n",
        "\n",
        "\n",
        "# print out the LLM response with the email content (RAG)\n",
        "# print(lookup_doc[bm25(\"Please give me key insight Trades on Nifty 50\", doc)[0][0]])\n",
        "query=\"Is there any illegal activities happening in enron?\"\n",
        "response = model.generate_content(lookup_doc[bm25(query, doc)[0][0]]+query)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YnqxBZ4gzTN"
      },
      "source": [
        "## Discussion (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyVyTcM-gzTN"
      },
      "source": [
        "In this section, reflect on the performance of different ranking methods and the impact of retrieval on LLM responses. Consider:\n",
        "\n",
        "- How did retrieval affect the LLM’s response? Did it improve factual accuracy or relevance?\n",
        "- Were there cases where retrieval hurt performance (e.g., irrelevant documents, redundancy)?\n",
        "- Any ideas for improving the ranking or retrieval process?\n",
        "\n",
        "Keep your discussion *concise* and *insightful*, focusing on key takeaways from your experiments. Please answer in bullet points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMjckcmgzTN"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "* It specified the context to lookup for the corresponding query\n",
        "* It helps us in defining boundaries for the LLM, without confusing it much. So, it will help us increase the factual relevance when used properly.\n",
        "* Proper context is required for using RAG-LLM.\n",
        "* We can provide filters to eliminate similar documents, this will help us diversify the Data Context.\n",
        "* if the context is irrelevant to the query, it will affect the performance of LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FihN3qniN-Jy"
      },
      "source": [
        "# Collaboration Declarations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKp_Fe-N-Jy"
      },
      "source": [
        "*You should fill out your collaboration declarations here.*\n",
        "\n",
        "https://stackoverflow.com/questions/12897374/get-unique-values-from-a-list-in-python\n",
        "\n",
        "\n",
        "https://www.geeksforgeeks.org/python-difference-two-lists/\n",
        "\n",
        "ChatGPT:\n",
        "- i want to no. of occurences of each element in a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ASjn0YYqcAr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}