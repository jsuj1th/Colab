{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsuj1th/Colab/blob/main/DL/CSCE_636_600_Spring_2025_Project_3_%2B_Sujith_Julakanti_%2B_335007274.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9-sOeWSh4xg",
        "outputId": "ada11fc0-6f1a-4f0c-fb31-3774ea4e79f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tamu_csce_636_project1\n",
            "  Downloading tamu_csce_636_project1-0.0.7-py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading tamu_csce_636_project1-0.0.7-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: tamu_csce_636_project1\n",
            "Successfully installed tamu_csce_636_project1-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install tamu_csce_636_project1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset_generation:\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import linprog\n",
        "from itertools import combinations, product\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MPI setup\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Set a unique random seed per job and rank to ensure different matrix generation\n",
        "np.random.seed(3)\n",
        "\n",
        "output_dir = \"/scratch/user/haikookhandor/DL/datasets_mpi/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Parameters\n",
        "max_valid_samples = 10000 # total valid samples per rank for debugging\n",
        "save_threshold_bytes = 5 * 1024 * 1024\n",
        "\n",
        "# (n, k, m) combinations\n",
        "allowed_combinations = [\n",
        "    (9, 4, [2, 3,4,5]),\n",
        "    (9, 5, [2, 3,4]),\n",
        "    (9, 6, [2,3]),\n",
        "    (10, 4, [2, 3,4,5,6]),\n",
        "    (10, 5, [2, 3,4,5]),\n",
        "    (10, 6, [2,3,4]),\n",
        "]\n",
        "triplets = [(n, k, m) for n, k, ms in allowed_combinations for m in ms]\n",
        "\n",
        "# Divide combinations across ranks\n",
        "combo_chunks = np.array_split(triplets, size)\n",
        "my_combos = combo_chunks[rank]\n",
        "\n",
        "def generate_all_tuples(n, m):\n",
        "    for a in range(n):\n",
        "        for b in range(n):\n",
        "            if b == a:\n",
        "                continue\n",
        "            rest = [i for i in range(n) if i not in {a, b}]\n",
        "            for X in combinations(rest, m - 1):\n",
        "                for psi in product([-1, 1], repeat=m):\n",
        "                    yield (a, b, X, psi)\n",
        "\n",
        "def solve_lp(G, k, n, m, a, b, X, psi):\n",
        "    Y = [i for i in range(n) if i not in set(X).union({a, b})]\n",
        "    x_list = [a] + sorted(X) + [b] + sorted(Y)\n",
        "    tau_inv = {v: i for i, v in enumerate(x_list)}\n",
        "    c = -psi[0] * G[:, a]\n",
        "    A_ub, b_ub = [], []\n",
        "\n",
        "    for j in X:\n",
        "        row = (psi[tau_inv[j]] * G[:, j] - psi[0] * G[:, a])\n",
        "        A_ub.append(row)\n",
        "        b_ub.append(0)\n",
        "        row = -psi[tau_inv[j]] * G[:, j]\n",
        "        A_ub.append(row)\n",
        "        b_ub.append(-1)\n",
        "\n",
        "    for j in Y:\n",
        "        A_ub.append(G[:, j])\n",
        "        b_ub.append(1)\n",
        "        A_ub.append(-G[:, j])\n",
        "        b_ub.append(1)\n",
        "\n",
        "    A_eq = [G[:, b]]\n",
        "    b_eq = [1]\n",
        "\n",
        "    res = linprog(c, A_ub=np.array(A_ub), b_ub=np.array(b_ub),\n",
        "                  A_eq=np.array(A_eq), b_eq=np.array(b_eq),\n",
        "                  method='highs')\n",
        "\n",
        "    if res.status == 0:\n",
        "        return -res.fun\n",
        "    elif res.status == 3:\n",
        "        return np.inf\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def compute_m_height(G, k, n, m):\n",
        "    max_h = 1\n",
        "    for a, b, X, psi in generate_all_tuples(n, m):\n",
        "        h = solve_lp(G, k, n, m, a, b, X, psi)\n",
        "        if h == np.inf:\n",
        "            return np.inf\n",
        "        if h > max_h:\n",
        "            max_h = h\n",
        "    return max_h\n",
        "\n",
        "# Initialize global sample buffer\n",
        "samples = []\n",
        "file_counter = 0\n",
        "total_valid_samples = 0\n",
        "\n",
        "with tqdm(total=max_valid_samples, disable=(rank != 0)) as pbar:\n",
        "    while total_valid_samples < max_valid_samples:\n",
        "        for n, k, m in my_combos:\n",
        "            if total_valid_samples >= max_valid_samples:\n",
        "                break\n",
        "\n",
        "            P = np.random.uniform(-100, 100, size=(k, n - k))\n",
        "            G = np.hstack([np.eye(k), P])\n",
        "            h_m = compute_m_height(G, k, n, m)\n",
        "\n",
        "            if h_m != np.inf and h_m > 0:\n",
        "                samples.append({\n",
        "                    \"n\": n,\n",
        "                    \"k\": k,\n",
        "                    \"m\": m,\n",
        "                    \"h_m\": h_m,\n",
        "                    \"P\": P.flatten().tolist()\n",
        "                })\n",
        "                total_valid_samples += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "                if len(samples) >= 10:\n",
        "                    df = pd.DataFrame(samples)\n",
        "                    mem_size = df.memory_usage(deep=True).sum()\n",
        "                    if mem_size >= save_threshold_bytes:\n",
        "                        filename = f\"{output_dir}/rank{rank}_part{file_counter}.csv\"\n",
        "                        df.to_csv(filename, index=False)\n",
        "                        print(f\"[Rank {rank}] Saved {filename} ({mem_size / (1024**2):.2f} MB)\")\n",
        "                        file_counter += 1\n",
        "                        samples = []\n",
        "\n",
        "# Save any remaining samples\n",
        "if samples:\n",
        "    df = pd.DataFrame(samples)\n",
        "    filename = f\"{output_dir}/rank{rank}_final.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"[Rank {rank}] Saved {filename} (final)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "U9c_cckth-7g",
        "outputId": "f04528d2-95e9-41f3-a40f-5f4afb7b5b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mpi4py'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0870e9355d28>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpi4py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpi4py'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "import csv\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.linalg import svd, norm, cond\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "INPUT_PATH = '/Users/sujithjulakanti/Desktop/Colab/DL/DL_Project/results_dataframe.pkl'\n",
        "MODEL_SAVE_DIR = 'model_dir/saved_models_14'\n",
        "LOG_FILE = 'log_dir/training_progress_14.log'\n",
        "CSV_PATH = 'DL_APPROACH/pred_dir/test_predictions_14.csv'\n",
        "SCALER_PATH = 'DL_APPROACH/scaler_dir/scaler_14.pkl'\n",
        "\n",
        "os.makedirs(os.path.dirname(MODEL_SAVE_DIR), exist_ok=True)\n",
        "os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
        "os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n",
        "os.makedirs(os.path.dirname(SCALER_PATH), exist_ok=True)\n",
        "\n",
        "N_VAL, K_VAL, M_VAL = 10, 4, 5\n",
        "EXPECTED_P_LEN = 25\n",
        "\n",
        "SAVE_EVERY_N_EPOCHS = 10\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 100\n",
        "PATIENCE = 35\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# ---------------------------\n",
        "# Utility Functions\n",
        "# ---------------------------\n",
        "def load_and_filter_data(path: str, n_val: int, k_val: int, m_val: int) -> pd.DataFrame:\n",
        "    df = joblib.load(path)\n",
        "    print(f\"Loaded data: {df.shape}\")\n",
        "    df = df[(df['n'] == n_val) & (df['k'] == k_val) & (df['m'] == m_val)]\n",
        "    if isinstance(df['P'].iloc[0], str):\n",
        "        df['P'] = df['P'].apply(ast.literal_eval)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    max_nk = max((df['n'] - df['k']) * df['k'])\n",
        "    X, y = [], df['result'].values.astype(np.float32)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        n, k, m = int(row['n']), int(row['k']), int(row['m'])\n",
        "        P_matrix = np.array(row['P'], dtype=np.float32).reshape(k, n - k)\n",
        "\n",
        "        padded_P = np.zeros((k, n - k), dtype=np.float32)\n",
        "        padded_P[:P_matrix.shape[0], :P_matrix.shape[1]] = P_matrix\n",
        "\n",
        "        features = np.concatenate([\n",
        "            np.array([n, k, m], dtype=np.float32),\n",
        "            padded_P.flatten(),\n",
        "            np.mean(padded_P, axis=1), np.std(padded_P, axis=1),\n",
        "            np.mean(padded_P, axis=0), np.std(padded_P, axis=0),\n",
        "            np.linalg.norm(padded_P, axis=1),\n",
        "            [norm(padded_P, ord='fro'), cond(padded_P) if np.linalg.cond(padded_P) else 1e6],\n",
        "            np.pad(svd(padded_P, full_matrices=False)[1][:5], (0, 5 - len(svd(padded_P, full_matrices=False)[1])), constant_values=0)\n",
        "        ])\n",
        "        X.append(features)\n",
        "    return np.array(X, dtype=np.float32), y\n",
        "\n",
        "def save_scaler(X: np.ndarray, path: str) -> np.ndarray:\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    joblib.dump(scaler, path)\n",
        "    return X_scaled\n",
        "\n",
        "def prepare_dataloaders(X: np.ndarray, y: np.ndarray) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train).unsqueeze(1)), batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val).unsqueeze(1)), batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test).unsqueeze(1)), batch_size=BATCH_SIZE)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "class MHeightRegressor(nn.Module):\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
        "            nn.Linear(256, 200), nn.BatchNorm1d(200), nn.ReLU(),\n",
        "            nn.Linear(200, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
        "            nn.Linear(32, 16), nn.BatchNorm1d(16), nn.ReLU(),\n",
        "            nn.Linear(16, 1), nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x) + 1\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    loss_total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = torch.log2(model(xb))\n",
        "            ylog = torch.log2(yb)\n",
        "            loss_total += criterion(preds, ylog).item() * xb.size(0)\n",
        "    return loss_total / len(loader.dataset)\n",
        "\n",
        "def save_predictions(preds: list, targets: list, inputs: list, csv_path: str):\n",
        "    header = ['n', 'k', 'm'] + [f'P{i}' for i in range(EXPECTED_P_LEN)] + ['true_result', 'predicted_result']\n",
        "    with open(csv_path, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        for x_vec, y_true, y_pred in zip(inputs, targets, preds):\n",
        "            writer.writerow(list(x_vec[:3]) + list(x_vec[3:]) + [y_true, y_pred])\n",
        "\n",
        "    simple_csv_path = csv_path.replace('.csv', '_simple.csv')\n",
        "    with open(simple_csv_path, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['true_result', 'predicted_result'])\n",
        "        for y_true, y_pred in zip(targets, preds):\n",
        "            writer.writerow([y_true, y_pred])\n",
        "    print(f\"Predictions saved to {csv_path} and {simple_csv_path}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Main Execution\n",
        "# ---------------------------\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "df = load_and_filter_data(INPUT_PATH, N_VAL, K_VAL, M_VAL)\n",
        "X, y = preprocess_data(df)\n",
        "X_scaled = save_scaler(X, SCALER_PATH)\n",
        "train_loader, val_loader, test_loader = prepare_dataloaders(X_scaled, y)\n",
        "\n",
        "input_dim = X_scaled.shape[1]\n",
        "model = MHeightRegressor(input_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "\n",
        "with open(LOG_FILE, 'w') as f:\n",
        "    f.write(f\"Training started at {time.ctime()}\\n\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(torch.log2(model(xb)), torch.log2(yb))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch}: Validation Loss = {val_loss:.6f}\")\n",
        "    with open(LOG_FILE, 'a') as f:\n",
        "        f.write(f\"Epoch {epoch}: Validation Loss = {val_loss:.6f}\\n\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, 'best_model_13-2.pt'))\n",
        "    elif epoch - best_epoch >= PATIENCE:\n",
        "        print(f\"Early stopping at epoch {epoch}. Best epoch was {best_epoch}.\")\n",
        "        break\n",
        "\n",
        "    if epoch % SAVE_EVERY_N_EPOCHS == 0:\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, f'model_epoch{epoch}.pt'))\n",
        "\n",
        "# ---------------------------\n",
        "# Final Evaluation\n",
        "# ---------------------------\n",
        "model.load_state_dict(torch.load(os.path.join(MODEL_SAVE_DIR, 'best_model_14.pt')))\n",
        "model.eval()\n",
        "\n",
        "all_preds, all_targets, all_inputs = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        preds = torch.log2(model(xb)).cpu()\n",
        "        ylog = torch.log2(yb)\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_targets.extend(ylog.flatten())\n",
        "        all_inputs.extend(xb.cpu().numpy())\n",
        "\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "mae = mean_absolute_error(all_targets, all_preds)\n",
        "mse = mean_squared_error(all_preds, all_targets)\n",
        "test_loss = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "with open(LOG_FILE, 'a') as f:\n",
        "    f.write(f\"Training ended at {time.ctime()}\\n\")\n",
        "    f.write(f\"Best Epoch: {best_epoch} | Best Val Loss: {best_val_loss:.6f}\\n\")\n",
        "    f.write(f\"Test Loss: {test_loss:.6f} | R² Score: {r2:.4f} | MAE: {mae:.4f} | MSE: {mse:.4f}\\n\")\n",
        "\n",
        "save_predictions(all_preds, all_targets, all_inputs, CSV_PATH)\n",
        "\n",
        "print(f\"Test Results -- Loss: {test_loss:.6f}, R2: {r2:.4f}, MAE: {mae:.4f}, MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "rD_yi-2riFFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tamu_csce_636_project1 import Evaluator\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import joblib\n",
        "from numpy.linalg import svd, norm, cond\n",
        "from typing import List, Tuple\n",
        "# Initialize the evaluator\n",
        "evaluator = Evaluator(\n",
        "    first_name=\"Sujith\",\n",
        "    last_name=\"Julakantu\",\n",
        "    email=\"s02@tamu.edu\",\n",
        "    print=False,\n",
        ")\n",
        "# === Paths ===\n",
        "MODEL_WEIGHTS_PATH = '/content/best_model_14.pt'\n",
        "SCALER_PATH = '/content/scaler_14.pkl'\n",
        "\n",
        "# === Define the model architecture (must match training exactly) ===\n",
        "class MHeightRegressor(nn.Module):\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
        "            nn.Linear(256, 200), nn.BatchNorm1d(200), nn.ReLU(),\n",
        "            nn.Linear(200, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
        "            nn.Linear(32, 16), nn.BatchNorm1d(16), nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x) + 1\n",
        "\n",
        "# === Preprocessing function for new test samples ===\n",
        "def preprocess_new_data(samples: List[Tuple[int, int, int, np.ndarray]]) -> np.ndarray:\n",
        "    features = []\n",
        "    TARGET_FEATURE_SIZE = 58  # Always create 58 features\n",
        "\n",
        "    for n, k, m, P_matrix in samples:\n",
        "        P_matrix = np.array(P_matrix, dtype=np.float32).reshape(k, n - k)\n",
        "\n",
        "        padded_P = np.zeros((k, n - k), dtype=np.float32)\n",
        "        padded_P[:P_matrix.shape[0], :P_matrix.shape[1]] = P_matrix\n",
        "\n",
        "        basic_features = np.array([n, k, m], dtype=np.float32)\n",
        "        flat_P = padded_P.flatten()\n",
        "        row_mean = np.mean(padded_P, axis=1)\n",
        "        row_std = np.std(padded_P, axis=1)\n",
        "        col_mean = np.mean(padded_P, axis=0)\n",
        "        col_std = np.std(padded_P, axis=0)\n",
        "        row_l2 = np.linalg.norm(padded_P, axis=1)\n",
        "        frob_norm = norm(padded_P, ord='fro')\n",
        "\n",
        "        try:\n",
        "            cond_number = cond(padded_P)\n",
        "        except np.linalg.LinAlgError:\n",
        "            cond_number = 1e6\n",
        "\n",
        "        try:\n",
        "            U, s_vals, Vt = svd(padded_P, full_matrices=False)\n",
        "        except np.linalg.LinAlgError:\n",
        "            s_vals = np.zeros(min(k, n - k))\n",
        "\n",
        "        top_singulars = np.pad(s_vals[:5], (0, 5 - len(s_vals)), constant_values=0)\n",
        "\n",
        "        feature_vec = np.concatenate([\n",
        "            basic_features,\n",
        "            flat_P,\n",
        "            row_mean, row_std,\n",
        "            col_mean, col_std,\n",
        "            row_l2,\n",
        "            [frob_norm, cond_number],\n",
        "            top_singulars\n",
        "        ])\n",
        "\n",
        "        # === Important Padding Step ===\n",
        "        if feature_vec.shape[0] < TARGET_FEATURE_SIZE:\n",
        "            pad_width = TARGET_FEATURE_SIZE - feature_vec.shape[0]\n",
        "            feature_vec = np.pad(feature_vec, (0, pad_width), constant_values=0)\n",
        "        elif feature_vec.shape[0] > TARGET_FEATURE_SIZE:\n",
        "            feature_vec = feature_vec[:TARGET_FEATURE_SIZE]  # Trim if somehow too long\n",
        "\n",
        "        features.append(feature_vec)\n",
        "\n",
        "    return np.array(features, dtype=np.float32)\n",
        "\n",
        "# === Load the scaler and model ===\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "# Build dummy input to get correct input dimension\n",
        "dummy_sample = (10, 4, 5, np.random.rand(4, 6))\n",
        "dummy_features = preprocess_new_data([dummy_sample])\n",
        "input_dim = dummy_features.shape[1]\n",
        "\n",
        "model = MHeightRegressor(input_dim)\n",
        "model.load_state_dict(torch.load(MODEL_WEIGHTS_PATH, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# === Prediction function ===\n",
        "def predict_m_heights(n: int, k: int, m: int, list_of_P_matrices: List[np.ndarray]) -> torch.Tensor:\n",
        "    samples = []\n",
        "    for P_matrix in list_of_P_matrices:\n",
        "        samples.append((n, k, m, P_matrix))\n",
        "\n",
        "    X_new = preprocess_new_data(samples)\n",
        "    X_scaled = scaler.transform(X_new)\n",
        "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = model(X_tensor).squeeze(1)\n",
        "\n",
        "    return preds.cpu().numpy().tolist()  # Evaluator expects pure Python (not torch.Tensor)\n",
        "\n",
        "σ = evaluator.eval(\n",
        "    inputs={\n",
        "        '[5,2,2]': [\n",
        "            np.array([\n",
        "                [0.4759809, 0.9938236, 0.819425],\n",
        "                [-0.8960798, -0.7442706, 0.3345122],\n",
        "            ]),\n",
        "        ],\n",
        "    },\n",
        "    outputs={\n",
        "        '[5,2,2]': [1.9242387],\n",
        "    },\n",
        "    func=predict_m_heights,\n",
        ")\n",
        "\n",
        "# Print evaluation result\n",
        "print(\"Evaluation result:\", dict(σ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7_gwxfdlku3",
        "outputId": "8e670ec9-ce9b-4a35-8d2b-f4b927018d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation result: {(5, 2, 2): 83.5453074185373}\n"
          ]
        }
      ]
    }
  ]
}